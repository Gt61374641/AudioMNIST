{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FWumlmcL3r0a",
        "pqJrjauP846-",
        "DGvexk_lvq1b",
        "og7YgBCLct5T",
        "Wb_jgNjdurVl",
        "vDxQ-_Ao7NBG",
        "QJqa5zAobgr2",
        "IqNWFF1wyZtE",
        "EJbgqIFEvty2",
        "WrXjUZiI5oNH",
        "P5KsYCx34JdT",
        "PdPUGj0iCqX9",
        "hGZXjnnfiFU6",
        "HICnX_kEQDhR",
        "CKl_IU1q4yeE",
        "WDa_hakb42II",
        "ieuJPBse44cA"
      ],
      "gpuType": "T4"
    },
    "interpreter": {
      "hash": "26284b83044cf6d18497d2f2bcb8891a6475cbd87428bcdc4981d4fbeeb38628"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWumlmcL3r0a"
      },
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfE3V9Af9v5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13fed805-a528-4aed-9f65-a6ed0f4269a1"
      },
      "source": [
        "# Import packages\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import pathlib\n",
        "import os\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.losses import *\n",
        "import requests\n",
        "!pip install pyunpack patool\n",
        "import pyunpack\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import sys\n",
        "\n",
        "!pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray\n",
        "from rarfile import RarFile\n",
        "import segmentation_models as sm\n",
        "from keras_segmentation.models import segnet\n",
        "from keras.applications import vgg16\n",
        "from sklearn.metrics import *\n",
        "import rioxarray as rxr\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyunpack in /usr/local/lib/python3.12/dist-packages (0.3)\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: easyprocess in /usr/local/lib/python3.12/dist-packages (from pyunpack) (1.1)\n",
            "Requirement already satisfied: entrypoint2 in /usr/local/lib/python3.12/dist-packages (from pyunpack) (1.1)\n",
            "Collecting git+https://github.com/davej23/image-segmentation-keras.git\n",
            "  Cloning https://github.com/davej23/image-segmentation-keras.git to /tmp/pip-req-build-x96z0aam\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/davej23/image-segmentation-keras.git /tmp/pip-req-build-x96z0aam\n",
            "  Resolved https://github.com/davej23/image-segmentation-keras.git to commit e01b0a8d5859854cd9d259a618829889166439f5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.12/dist-packages (4.2)\n",
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: rioxarray in /usr/local/lib/python3.12/dist-packages (0.20.0)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.12/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.12/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.25.2)\n",
            "Collecting h5py<=2.10.0 (from keras_segmentation==0.3.0)\n",
            "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Keras>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from keras_segmentation==0.3.0) (3.10.0)\n",
            "Collecting imageio==2.5.0 (from keras_segmentation==0.3.0)\n",
            "  Using cached imageio-2.5.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting imgaug>=0.4.0 (from keras_segmentation==0.3.0)\n",
            "  Using cached imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from keras_segmentation==0.3.0) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from keras_segmentation==0.3.0) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imageio==2.5.0->keras_segmentation==0.3.0) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imageio==2.5.0->keras_segmentation==0.3.0) (11.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from rioxarray) (25.0)\n",
            "Requirement already satisfied: rasterio>=1.4.3 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (1.4.3)\n",
            "Requirement already satisfied: xarray>=2024.7.0 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (2025.11.0)\n",
            "Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (3.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from h5py<=2.10.0->keras_segmentation==0.3.0) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (3.10.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (2.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (0.5.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj>=3.3->rioxarray) (2025.11.12)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (25.4.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (8.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (0.7.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (1.1.1.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (3.2.5)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.12/dist-packages (from xarray>=2024.7.0->rioxarray) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray>=2024.7.0->rioxarray) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray>=2024.7.0->rioxarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray>=2024.7.0->rioxarray) (2025.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.6)\n",
            "INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit-image (from efficientnet==1.0.0->segmentation-models)\n",
            "  Using cached scikit_image-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.25.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.23.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.23.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "  Using cached scikit_image-0.21.0.tar.gz (22.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is still looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached scikit_image-0.20.0-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2025.10.16)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.9.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (1.4.9)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->Keras>=2.0.0->keras_segmentation==0.3.0) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->Keras>=2.0.0->keras_segmentation==0.3.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->Keras>=2.0.0->keras_segmentation==0.3.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->Keras>=2.0.0->keras_segmentation==0.3.0) (0.1.2)\n",
            "Using cached imageio-2.5.0-py3-none-any.whl (3.3 MB)\n",
            "Using cached imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "Building wheels for collected packages: keras_segmentation, h5py\n",
            "  Building wheel for keras_segmentation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34363 sha256=5fe1ec3defeab6f7afa0dc8d086a341ad7d8a283999842c90e730758b9754afe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-039ogrcw/wheels/5d/b4/9f/fd146ed9c93031be7aec7f6147a4f17279198369e2011ef5a0\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for h5py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for h5py\n",
            "Successfully built keras_segmentation\n",
            "Failed to build h5py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (h5py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras.utils' has no attribute 'generic_utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-198866628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install rarfile segmentation-models git+https://github.com/davej23/image-segmentation-keras.git rioxarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrarfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRarFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation_models\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_segmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/segmentation_models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0m_framework\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SM_FRAMEWORK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DEFAULT_KERAS_FRAMEWORK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mset_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_framework\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TF_KERAS_FRAMEWORK_NAME\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_framework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_KERAS_FRAMEWORK_NAME\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_KERAS_FRAMEWORK_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/segmentation_models/__init__.py\u001b[0m in \u001b[0;36mset_framework\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_KERAS_FRAMEWORK_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mefficientnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m  \u001b[0;31m# init custom objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TF_KERAS_FRAMEWORK_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/efficientnet/keras.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpreprocess_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minject_keras_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0minit_keras_custom_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/efficientnet/__init__.py\u001b[0m in \u001b[0;36minit_keras_custom_objects\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     }\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_custom_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.utils' has no attribute 'generic_utils'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyunpack patool rarfile rioxarray scikit-learn --prefer-binary\n",
        "!pip install segmentation-models\n",
        "!pip install git+https://github.com/divamgupta/image-segmentation-keras.git\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from pyunpack import Archive\n",
        "from rarfile import RarFile\n",
        "import rioxarray as rxr\n",
        "from sklearn.metrics import *\n",
        "\n",
        "# Required setting for segmentation-models\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Fix for AttributeError: module 'keras.utils' has no attribute 'generic_utils'\n",
        "import tensorflow.keras.utils as k_utils\n",
        "if not hasattr(k_utils, 'generic_utils'):\n",
        "    k_utils.generic_utils = k_utils\n",
        "\n",
        "import segmentation_models as sm\n",
        "\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import vgg16\n",
        "\n",
        "from keras_segmentation.models import segnet\n",
        "\n",
        "print(\"Environment setup complete.\")\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mD_geEBNkNHC",
        "outputId": "7b2087d0-6c0c-47bb-d046-3db62e8e32a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyunpack in /usr/local/lib/python3.12/dist-packages (0.3)\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.12/dist-packages (4.2)\n",
            "Requirement already satisfied: rioxarray in /usr/local/lib/python3.12/dist-packages (0.20.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: easyprocess in /usr/local/lib/python3.12/dist-packages (from pyunpack) (1.1)\n",
            "Requirement already satisfied: entrypoint2 in /usr/local/lib/python3.12/dist-packages (from pyunpack) (1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from rioxarray) (25.0)\n",
            "Requirement already satisfied: rasterio>=1.4.3 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (1.4.3)\n",
            "Requirement already satisfied: xarray>=2024.7.0 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (2025.11.0)\n",
            "Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (3.7.2)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from rioxarray) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj>=3.3->rioxarray) (2025.11.12)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (25.4.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (8.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (0.7.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (1.1.1.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->rioxarray) (3.2.5)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.12/dist-packages (from xarray>=2024.7.0->rioxarray) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray>=2024.7.0->rioxarray) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray>=2024.7.0->rioxarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2->xarray>=2024.7.0->rioxarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2->xarray>=2024.7.0->rioxarray) (1.17.0)\n",
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.12/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.12/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.15.1)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.6)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (0.4)\n",
            "Collecting git+https://github.com/divamgupta/image-segmentation-keras.git\n",
            "  Cloning https://github.com/divamgupta/image-segmentation-keras.git to /tmp/pip-req-build-i2rmi4k2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/divamgupta/image-segmentation-keras.git /tmp/pip-req-build-i2rmi4k2\n",
            "  Resolved https://github.com/divamgupta/image-segmentation-keras.git to commit 1b2ba53ae49387c2d1abbd9a2f4a9a45eea6912f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h5py<=2.10.0 (from keras_segmentation==0.3.0)\n",
            "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Keras>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from keras_segmentation==0.3.0) (3.10.0)\n",
            "Collecting imageio==2.5.0 (from keras_segmentation==0.3.0)\n",
            "  Using cached imageio-2.5.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting imgaug>=0.4.0 (from keras_segmentation==0.3.0)\n",
            "  Using cached imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from keras_segmentation==0.3.0) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from keras_segmentation==0.3.0) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imageio==2.5.0->keras_segmentation==0.3.0) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imageio==2.5.0->keras_segmentation==0.3.0) (11.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from h5py<=2.10.0->keras_segmentation==0.3.0) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (0.25.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from imgaug>=0.4.0->keras_segmentation==0.3.0) (2.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from Keras>=2.0.0->keras_segmentation==0.3.0) (25.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->keras_segmentation==0.3.0) (3.6)\n",
            "INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit-image>=0.14.2 (from imgaug>=0.4.0->keras_segmentation==0.3.0)\n",
            "  Using cached scikit_image-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.25.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.23.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.23.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "  Using cached scikit_image-0.21.0.tar.gz (22.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is still looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached scikit_image-0.20.0-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->keras_segmentation==0.3.0) (2025.10.16)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->keras_segmentation==0.3.0) (1.9.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->keras_segmentation==0.3.0) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug>=0.4.0->keras_segmentation==0.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->Keras>=2.0.0->keras_segmentation==0.3.0) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->Keras>=2.0.0->keras_segmentation==0.3.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->Keras>=2.0.0->keras_segmentation==0.3.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->Keras>=2.0.0->keras_segmentation==0.3.0) (0.1.2)\n",
            "Using cached imageio-2.5.0-py3-none-any.whl (3.3 MB)\n",
            "Using cached imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "Building wheels for collected packages: keras_segmentation, h5py\n",
            "  Building wheel for keras_segmentation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34584 sha256=a0b33f2fc874c556a53f87c9f9dd652da2d47390ba93ed19f62c4ae69b49b538\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-33e15w7b/wheels/ca/55/30/2ac53889fac36153a6f34e8244d27708f2d99ec0b6eee52f59\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for h5py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for h5py\n",
            "Successfully built keras_segmentation\n",
            "Failed to build h5py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (h5py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras_segmentation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2220597051.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_segmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Environment setup complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_segmentation'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOqejGEXQLZ2"
      },
      "source": [
        "# Specify whether to download data or read in\n",
        "download = True\n",
        "base_dir = r\"./Amazon Forest Dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjNYvVXlC6Ja"
      },
      "source": [
        "# Download data\n",
        "\n",
        "if download:\n",
        "    url = 'https://zenodo.org/record/3233081/files/Amazon%20Forest%20Dataset.rar?download=1'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('data.rar', 'wb').write(r.content)\n",
        "\n",
        "    if sys.platform != 'darwin':\n",
        "        pyunpack.Archive('data.rar').extractall('')\n",
        "\n",
        "    else:\n",
        "        with RarFile('data.rar') as rf:\n",
        "            rf.extractall()\n",
        "\n",
        "base_dir = r\"./Amazon Forest Dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK90NPEP99zG"
      },
      "source": [
        "# Show example image from training data\n",
        "PIL.Image.open(r\"{}Training/images/Amazon_1110.tiff_25.tiff\".format(base_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8Iv5caGLEe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqJrjauP846-"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTEOTi_qwD0G"
      },
      "source": [
        "'''\n",
        "  Returns an image plot of mask prediction\n",
        "'''\n",
        "\n",
        "def reconstruct_image(model, image, rounded=False):\n",
        "\n",
        "  # Find model prediction\n",
        "  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n",
        "  # Standardise between 0-1\n",
        "  reconstruction = reconstruction/np.max(reconstruction)\n",
        "\n",
        "  # Round to 0-1, binary pixel-by-pixel classification\n",
        "  if rounded:\n",
        "    reconstruction = np.round(reconstruction)\n",
        "\n",
        "  # Plot reconstructed mask (prediction)\n",
        "  plt.imshow(reconstruction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxUiu0_OxMlo"
      },
      "source": [
        "'''\n",
        "  Returns array of mask prediction, given model and image\n",
        "'''\n",
        "def reconstruct_array(model, image, rounded=False):\n",
        "\n",
        "  # Find model prediction\n",
        "  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n",
        "\n",
        "  if rounded:\n",
        "    reconstruction = np.round(reconstruction)\n",
        "\n",
        "  return reconstruction # Returns array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prjIhHN-r4o5"
      },
      "source": [
        "'''\n",
        "  Metric functions for evaluation\n",
        "'''\n",
        "\n",
        "def score_eval(model, image, mask): # Gives score of mask vs prediction\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return accuracy_score(mask.flatten(), reconstruction)\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    scores = []\n",
        "    for i in range(len(image)):\n",
        "      reconstruction = model.predict(image[i].reshape(1, 512, 512, 3))\n",
        "      reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def score_eval2(model, image, mask): # Gives score of mask vs prediction\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return accuracy_score(mask.flatten(), reconstruction)\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    scores = []\n",
        "    for i in range(len(image)):\n",
        "      reconstruction = model.predict(image[i].reshape(1, 512, 512, 4))\n",
        "      reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def recall_eval(model, image, mask): # Find recall score\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return recall_score(mask.flatten(), reconstruction, average='weighted')\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    recall = []\n",
        "    for i in range(len(image)):\n",
        "        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n",
        "        reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "        recall.append(recall_score(mask[i].flatten(), reconstruction, average='weighted'))\n",
        "\n",
        "    return recall\n",
        "\n",
        "def precision_eval(model, image, mask): # Find precision score\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return precision_score(mask.flatten(), reconstruction, average='weighted')\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    precision = []\n",
        "    for i in range(len(image)):\n",
        "        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n",
        "        reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "        precision.append(precision_score(mask[i].flatten(), reconstruction, average='weighted'))\n",
        "\n",
        "    return precision\n",
        "\n",
        "def f1_score_eval(model, image, mask): # Find F1-score\n",
        "    prec = np.mean(precision_eval(model, image, mask))\n",
        "    rec = np.mean(recall_eval(model, image, mask))\n",
        "\n",
        "    if prec + rec == 0:\n",
        "        return 0\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def f1_score_eval_basic(precision, recall):\n",
        "    prec = np.mean(precision)\n",
        "    rec = np.mean(recall)\n",
        "\n",
        "    if prec + rec == 0:\n",
        "        return 0\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def produce_mask(image): # Outputs rounded image (binary)\n",
        "  return np.round(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGvexk_lvq1b"
      },
      "source": [
        "# Ingest and Process RGB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLRxMrWmqaHw"
      },
      "source": [
        "# Ingest images\n",
        "\n",
        "## Training images\n",
        "training_images_list = os.listdir(r\"{}Training/images/\".format(base_dir))\n",
        "training_masks_list = []\n",
        "training_images = []\n",
        "for n in training_images_list:\n",
        "  im = PIL.Image.open(r\"{}Training/images/{}\".format(base_dir,n))\n",
        "  training_images.append(im)\n",
        "  training_masks_list.append(n[:-5]+'.png')\n",
        "\n",
        "## Training masks\n",
        "training_masks = []\n",
        "for n in training_masks_list:\n",
        "  im = PIL.Image.open(r\"{}Training/masks/{}\".format(base_dir,n))\n",
        "  training_masks.append(im)\n",
        "\n",
        "## Test images\n",
        "test_images_list = os.listdir(r\"{}Test/\".format(base_dir))\n",
        "test_images = []\n",
        "for n in test_images_list:\n",
        "  im = PIL.Image.open(r\"{}Test/{}\".format(base_dir,n))\n",
        "  test_images.append(im)\n",
        "\n",
        "## Validation images\n",
        "validation_images_list = os.listdir(r\"{}Validation/images/\".format(base_dir))\n",
        "validation_masks_list = []\n",
        "validation_images = []\n",
        "for n in validation_images_list:\n",
        "  im = PIL.Image.open(r\"{}Validation/images/{}\".format(base_dir,n))\n",
        "  validation_images.append(im)\n",
        "  validation_masks_list.append(n[:-5]+'.png')\n",
        "\n",
        "## Validation masks\n",
        "validation_masks = []\n",
        "for n in validation_masks_list:\n",
        "  im = PIL.Image.open(r\"{}Validation/masks/{}\".format(base_dir,n))\n",
        "  validation_masks.append(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAGhNiaR0W98"
      },
      "source": [
        "# Pre-process data, normalise and reshape\n",
        "for i in range(len(training_images)):\n",
        "  training_images[i] = np.array(training_images[i])/255\n",
        "  training_images[i] = training_images[i].reshape(512,512,3)\n",
        "  training_images[i] = training_images[i].astype('float32')\n",
        "\n",
        "for i in range(len(training_masks)):\n",
        "  training_masks[i] = (np.array(training_masks[i])-1)\n",
        "  training_masks[i] = training_masks[i][:512,:512]\n",
        "  training_masks[i] = training_masks[i].reshape(512,512,1)\n",
        "  training_masks[i] = training_masks[i].astype('int')\n",
        "\n",
        "for i in range(len(validation_images)):\n",
        "  validation_images[i] = np.array(validation_images[i])/255\n",
        "  validation_images[i] = validation_images[i].reshape(1,512,512,3)\n",
        "  validation_images[i] = validation_images[i].astype('float32')\n",
        "\n",
        "for i in range(len(validation_masks)):\n",
        "  validation_masks[i] = np.array(validation_masks[i])-1\n",
        "  validation_masks[i] = validation_masks[i][:512,:512]\n",
        "  validation_masks[i] = validation_masks[i].reshape(1,512,512,1)\n",
        "  validation_masks[i] = validation_masks[i].astype('int')\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "  test_images[i] = np.array(test_images[i])/255\n",
        "  test_images[i] = test_images[i].reshape(1,512,512,3)\n",
        "  test_images[i] = test_images[i].astype('float32')\n",
        "\n",
        "# Add some training images to validation data to increase size of validation set\n",
        "for i in range(25,30):\n",
        "  validation_images.append(training_images[i].reshape(1,512,512,3))\n",
        "  validation_masks.append(training_masks[i].reshape(1,512,512,1))\n",
        "\n",
        "# Remove five images from training data, which has been added to validation data\n",
        "training_images = training_images[0:25]\n",
        "training_masks = training_masks[0:25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aMREQdk9_9R"
      },
      "source": [
        "# Create TensorFlow datasets for validation sets\n",
        "validation_df = tf.data.Dataset.from_tensor_slices((validation_images, validation_masks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyaBlceQjvly"
      },
      "source": [
        "#\n",
        "# Data loader/generator from: https://github.com/bragagnololu/UNet-defmapping.git\n",
        "#\n",
        "\n",
        "def adjustData(img, mask, num_class):\n",
        "\n",
        "    mask[mask > 0.5] = 1 # FOREST\n",
        "    mask[mask <= 0.5] = 0 # NON-FOREST\n",
        "\n",
        "    return (img,mask)\n",
        "\n",
        "def trainGenerator(batch_size,\n",
        "                   image_array,\n",
        "                   mask_array,\n",
        "                   aug_dict,\n",
        "                   image_save_prefix  = \"image\",\n",
        "                   mask_save_prefix  = \"mask\",\n",
        "                   num_class = 2,\n",
        "                   save_to_dir = None,\n",
        "                   target_size = (512,512),\n",
        "                   seed = 1):\n",
        "\n",
        "    image_datagen = ImageDataGenerator(**aug_dict)\n",
        "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "\n",
        "    image_generator = image_datagen.flow(image_array,\n",
        "                                           batch_size = batch_size,\n",
        "                                           save_to_dir = save_to_dir,\n",
        "                                           save_prefix = image_save_prefix,\n",
        "                                           seed = seed)\n",
        "\n",
        "    mask_generator = mask_datagen.flow(mask_array,\n",
        "                                           batch_size = batch_size,\n",
        "                                           save_to_dir = save_to_dir,\n",
        "                                           save_prefix = mask_save_prefix,\n",
        "                                           seed = seed)\n",
        "\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "\n",
        "    for (img,mask) in train_generator:\n",
        "        img, mask = adjustData(img, mask, num_class)\n",
        "        yield (img, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAJgv7qOkgzA"
      },
      "source": [
        "#\n",
        "# Produce generators for training images\n",
        "#\n",
        "\n",
        "t_images = np.stack(training_images)\n",
        "t_masks = np.stack(training_masks)\n",
        "\n",
        "v_images = np.stack(validation_images)\n",
        "v_masks = np.stack(validation_masks)\n",
        "\n",
        "# Set parameters for data augmentation\n",
        "data_gen_args = dict(rotation_range=180,\n",
        "                    width_shift_range=0.25,\n",
        "                    height_shift_range=0.25,\n",
        "                    shear_range=0.25,\n",
        "                    zoom_range=0.25,\n",
        "                    horizontal_flip=True,\n",
        "                    vertical_flip = True,\n",
        "                    fill_mode='reflect',\n",
        "                    )\n",
        "\n",
        "train = trainGenerator(1, t_images, t_masks, data_gen_args, save_to_dir=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mz6o3t3sveL"
      },
      "source": [
        "# Ingest and Process 4-band Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og7YgBCLct5T"
      },
      "source": [
        "## 4-band Amazon dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCffD7yhLlXX"
      },
      "source": [
        "download = True # True, if files don't already exist in same directory\n",
        "base_dir2 = r\"./AMAZON/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov76wahQcvj_"
      },
      "source": [
        "# Download data (Amazon)\n",
        "\n",
        "if download:\n",
        "    url = 'https://zenodo.org/record/4498086/files/AMAZON.rar?download=1'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('data2.rar', 'wb').write(r.content)\n",
        "\n",
        "    if sys.platform != 'darwin':\n",
        "        pyunpack.Archive('data2.rar').extractall('')\n",
        "\n",
        "    else:\n",
        "        with RarFile('data2.rar') as rf:\n",
        "            rf.extractall()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blN04l8svxjo"
      },
      "source": [
        "# Ingest images and normalise\n",
        "\n",
        "## Training images\n",
        "training_images_list2 = os.listdir(r\"{}Training/image/\".format(base_dir2))[0:250]\n",
        "training_masks_list2 = []\n",
        "training_images2 = []\n",
        "for n in training_images_list2:\n",
        "  training_masks_list2.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir2,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  training_images2.append(a)\n",
        "\n",
        "## Training masks\n",
        "training_masks2 = []\n",
        "for n in training_masks_list2:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Training/label/{}\".format(base_dir2,n))))\n",
        "  training_masks2.append(a)\n",
        "\n",
        "## Test images\n",
        "test_images_list2 = os.listdir(r\"{}Test/image/\".format(base_dir2))\n",
        "test_masks_list2 = []\n",
        "test_images2 = []\n",
        "for n in test_images_list2:\n",
        "  test_masks_list2.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Test/image/{}\".format(base_dir2,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  test_images2.append(a)\n",
        "\n",
        "## Test masks\n",
        "test_masks2 = []\n",
        "for n in test_masks_list2:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Test/mask/{}\".format(base_dir2,n))))\n",
        "  test_masks2.append(a)\n",
        "\n",
        "## Validation images\n",
        "validation_images_list2 = os.listdir(r\"{}Validation/images/\".format(base_dir2))\n",
        "validation_masks_list2 = []\n",
        "validation_images2 = []\n",
        "for n in validation_images_list2:\n",
        "  validation_masks_list2.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Validation/images/{}\".format(base_dir2,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  validation_images2.append(a)\n",
        "\n",
        "## Validation masks\n",
        "validation_masks2 = []\n",
        "for n in validation_masks_list2:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Validation/masks/{}\".format(base_dir2,n))))\n",
        "  validation_masks2.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmgCg7ZZZdNr"
      },
      "source": [
        "# Show example train image\n",
        "plt.imshow((np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir2,training_images_list2[20])))[0,:,:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZCs99gac5i3"
      },
      "source": [
        "# Pre-process data, reshaping and transposing\n",
        "for i in range(len(training_images2)):\n",
        "  training_images2[i] = training_images2[i].astype('float32')\n",
        "  training_images2[i] = training_images2[i].T\n",
        "\n",
        "for i in range(len(training_masks2)):\n",
        "  training_masks2[i] = training_masks2[i].reshape(1,512,512,1)\n",
        "  training_masks2[i] = training_masks2[i].T\n",
        "\n",
        "for i in range(len(validation_images2)):\n",
        "  validation_images2[i] = validation_images2[i].astype('float32')\n",
        "  validation_images2[i] = validation_images2[i].T\n",
        "\n",
        "for i in range(len(validation_masks2)):\n",
        "  validation_masks2[i] = validation_masks2[i].reshape(1,512,512,1)\n",
        "  validation_masks2[i] = validation_masks2[i].T\n",
        "\n",
        "for i in range(len(test_images2)):\n",
        "  test_images2[i] = test_images2[i].astype('float32')\n",
        "  test_images2[i] = test_images2[i].T\n",
        "\n",
        "for i in range(len(test_masks2)):\n",
        "  test_masks2[i] = test_masks2[i].reshape(1,512,512,1)\n",
        "  test_masks2[i] = test_masks2[i].T\n",
        "\n",
        "for i in range(len(training_images2)):\n",
        "  training_images2[i] = training_images2[i].reshape(-1,512,512,4)\n",
        "\n",
        "for i in range(len(validation_images2)):\n",
        "  validation_images2[i] = validation_images2[i].reshape(-1,512,512,4)\n",
        "\n",
        "for i in range(len(test_images2)):\n",
        "  test_images2[i] = test_images2[i].reshape(-1,512,512,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZWqzWXDeHea"
      },
      "source": [
        "# Create TensorFlow datasets for training and validation sets\n",
        "train_df_4band_amazon = tf.data.Dataset.from_tensor_slices((training_images2[0:250], training_masks2[0:250]))\n",
        "validation_df_4band_amazon = tf.data.Dataset.from_tensor_slices((validation_images2, validation_masks2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb_jgNjdurVl"
      },
      "source": [
        "## 4-band Atlantic Forest dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBDbTSSzLuk4"
      },
      "source": [
        "download = True # True if files don't already exist in same directory\n",
        "base_dir3 = r\"./ATLANTIC FOREST/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr4p47-Kuvde"
      },
      "source": [
        "# Download data (Atlantic Forest)\n",
        "if download:\n",
        "    url = 'https://zenodo.org/record/4498086/files/ATLANTIC%20FOREST.rar?download=1'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('data3.rar', 'wb').write(r.content)\n",
        "\n",
        "    if sys.platform != 'darwin':\n",
        "        pyunpack.Archive('data3.rar').extractall('')\n",
        "\n",
        "    else:\n",
        "        with RarFile('data3.rar') as rf:\n",
        "            rf.extractall()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUhzzdQvOaL"
      },
      "source": [
        "# Ingest images and normalise\n",
        "\n",
        "## Training images\n",
        "training_images_list3 = os.listdir(r\"{}Training/image/\".format(base_dir3))[0:250]\n",
        "training_masks_list3 = []\n",
        "training_images3 = []\n",
        "for n in training_images_list3:\n",
        "  training_masks_list3.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir3,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  training_images3.append(a)\n",
        "\n",
        "## Training masks\n",
        "training_masks3 = []\n",
        "for n in training_masks_list3:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Training/label/{}\".format(base_dir3,n))))\n",
        "  training_masks3.append(a)\n",
        "\n",
        "## Test images\n",
        "test_images_list3 = os.listdir(r\"{}Test/image/\".format(base_dir3))\n",
        "test_masks_list3 = []\n",
        "test_images3 = []\n",
        "for n in test_images_list3:\n",
        "  test_masks_list3.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Test/image/{}\".format(base_dir3,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  test_images3.append(a)\n",
        "\n",
        "## Test masks\n",
        "test_masks3 = []\n",
        "for n in test_masks_list3:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Test/mask/{}\".format(base_dir3,n))))\n",
        "  test_masks3.append(a)\n",
        "\n",
        "## Validation images\n",
        "validation_images_list3 = os.listdir(r\"{}Validation/images/\".format(base_dir3))\n",
        "validation_masks_list3 = []\n",
        "validation_images3 = []\n",
        "for n in validation_images_list3:\n",
        "  validation_masks_list3.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Validation/images/{}\".format(base_dir3,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  validation_images3.append(a)\n",
        "\n",
        "## Validation masks\n",
        "validation_masks3 = []\n",
        "for n in validation_masks_list3:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Validation/masks/{}\".format(base_dir3,n))))\n",
        "  validation_masks3.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31I43OJxMVU"
      },
      "source": [
        "# Pre-process data, reshaping and transposing\n",
        "for i in range(len(training_images3)):\n",
        "  training_images3[i] = training_images3[i].astype('float32')\n",
        "  training_images3[i] = training_images3[i].T\n",
        "\n",
        "for i in range(len(training_masks3)):\n",
        "  training_masks3[i] = training_masks3[i].reshape(1,512,512,1)\n",
        "  training_masks3[i] = training_masks3[i].T\n",
        "\n",
        "for i in range(len(validation_images3)):\n",
        "  validation_images3[i] = validation_images3[i].astype('float32')\n",
        "  validation_images3[i] = validation_images3[i].T\n",
        "\n",
        "for i in range(len(validation_masks3)):\n",
        "  validation_masks3[i] = validation_masks3[i].reshape(1,512,512,1)\n",
        "  validation_masks3[i] = validation_masks3[i].T\n",
        "\n",
        "for i in range(len(test_images3)):\n",
        "  test_images3[i] = test_images3[i].astype('float32')\n",
        "  test_images3[i] = test_images3[i].T\n",
        "\n",
        "for i in range(len(test_masks3)):\n",
        "  test_masks3[i] = test_masks3[i].reshape(1,512,512,1)\n",
        "  test_masks3[i] = test_masks3[i].T\n",
        "\n",
        "\n",
        "for i in range(len(training_images3)):\n",
        "  training_images3[i] = training_images3[i].reshape(-1,512,512,4)\n",
        "\n",
        "for i in range(len(validation_images3)):\n",
        "  validation_images3[i] = validation_images3[i].reshape(-1,512,512,4)\n",
        "\n",
        "for i in range(len(test_images3)):\n",
        "  test_images3[i] = test_images3[i].reshape(-1,512,512,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn73cK3QE9fN"
      },
      "source": [
        "# Plot example training image first band\n",
        "plt.imshow(training_images3[0].reshape(512,512,4)[:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n-tBTsjyL5q"
      },
      "source": [
        "# Create TensorFlow datasets for training and validation sets\n",
        "train_df_4band_atlantic = tf.data.Dataset.from_tensor_slices((training_images3[0:250], training_masks3[0:250]))\n",
        "validation_df_4band_atlantic = tf.data.Dataset.from_tensor_slices((validation_images3, validation_masks3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCp3t2JSyQ9O"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDxQ-_Ao7NBG"
      },
      "source": [
        "## U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--tUx5qNR0Q4"
      },
      "source": [
        "'''\n",
        "  Convolutional block with set parameters and activation layer after\n",
        "'''\n",
        "\n",
        "def convBlock(input, filters, kernel, kernel_init='he_normal', act='relu', transpose=False):\n",
        "  if transpose == False:\n",
        "    #conv = ZeroPadding2D((1,1))(input)\n",
        "    conv = Conv2D(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "  else:\n",
        "    #conv = ZeroPadding2D((1,1))(input)\n",
        "    conv = Conv2DTranspose(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "\n",
        "  conv = Activation(act)(conv)\n",
        "  return conv\n",
        "\n",
        "'''\n",
        "  U-Net model\n",
        "'''\n",
        "\n",
        "def UNet(trained_weights = None, input_size = (512,512,3), drop_rate = 0.25, lr=0.0001):\n",
        "\n",
        "    ## Can add pretrained weights by specifying 'trained_weights'\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(input_size, batch_size=1)\n",
        "\n",
        "    ## Contraction phase\n",
        "    conv1 = convBlock(inputs, 64, 3)\n",
        "    conv1 = convBlock(conv1, 64, 3)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = convBlock(pool1, 128, 3)\n",
        "    conv2 = convBlock(conv2, 128, 3)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    #drop2 = Dropout(drop_rate)(pool2)\n",
        "\n",
        "    conv3 = convBlock(pool2, 256, 3)\n",
        "    conv3 = convBlock(conv3, 256, 3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    #drop3 = Dropout(drop_rate)(pool3)\n",
        "\n",
        "    conv4 = convBlock(pool3, 512, 3)\n",
        "    conv4 = convBlock(conv4, 512, 3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    #drop4 = Dropout(drop_rate)(pool4)\n",
        "\n",
        "    conv5 = convBlock(pool4, 1024, 3)\n",
        "    conv5 = convBlock(conv5, 1024, 3)\n",
        "\n",
        "    ## Expansion phase\n",
        "    up6 = (Conv2DTranspose(512, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv5))\n",
        "    merge6 = concatenate([conv4,up6])\n",
        "    conv6 = convBlock(merge6, 512, 3)\n",
        "    conv6 = convBlock(conv6, 512, 3)\n",
        "    #conv6 = Dropout(drop_rate)(conv6)\n",
        "\n",
        "    up7 = (Conv2DTranspose(256, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv6))\n",
        "    merge7 = concatenate([conv3,up7])\n",
        "    conv7 = convBlock(merge7, 256, 3)\n",
        "    conv7 = convBlock(conv7, 256, 3)\n",
        "    #conv7 = Dropout(drop_rate)(conv7)\n",
        "\n",
        "    up8 = (Conv2DTranspose(128, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv7))\n",
        "    merge8 = concatenate([conv2,up8])\n",
        "    conv8 = convBlock(merge8, 128, 3)\n",
        "    conv8 = convBlock(conv8, 128, 3)\n",
        "    #conv8 = Dropout(drop_rate)(conv8)\n",
        "\n",
        "    up9 = (Conv2DTranspose(64, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv8))\n",
        "    merge9 = concatenate([conv1,up9])\n",
        "    conv9 = convBlock(merge9, 64, 3)\n",
        "    conv9 = convBlock(conv9, 64, 3)\n",
        "\n",
        "    # Output layer\n",
        "    conv10 = convBlock(conv9, 1, 1, act='sigmoid')\n",
        "\n",
        "    model = Model(inputs, conv10)\n",
        "\n",
        "    model.compile(optimizer = adam_v2.Adam(learning_rate = lr), loss = 'binary_crossentropy', metrics = ['accuracy', 'mse'])\n",
        "\n",
        "    if trained_weights != None:\n",
        "    \tmodel.load_weights(trained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD6fODjlcLdG"
      },
      "source": [
        "# Print model layers and number of parameters\n",
        "UNet().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJqa5zAobgr2"
      },
      "source": [
        "## Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3uY93JpT_OZ"
      },
      "source": [
        "'''\n",
        "  Convolutional block with two conv layers and two activation layers\n",
        "'''\n",
        "\n",
        "def convBlock2(input, filters, kernel, kernel_init='he_normal', act='relu', transpose=False):\n",
        "  if transpose == False:\n",
        "    conv = Conv2D(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "    conv = Activation(act)(conv)\n",
        "    conv = Conv2D(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(conv)\n",
        "    conv = Activation(act)(conv)\n",
        "  else:\n",
        "    conv = Conv2DTranspose(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "    conv = Activation(act)(conv)\n",
        "    conv = Conv2DTranspose(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(conv)\n",
        "    conv = Activation(act)(conv)\n",
        "\n",
        "  return conv\n",
        "\n",
        "'''\n",
        "  Attention block/mechanism\n",
        "'''\n",
        "def attention_block(x, gating, inter_shape, drop_rate=0.25):\n",
        "\n",
        "    # Find shape of inputs\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "    ## Process x vector and gating signal\n",
        "    # x vector input and processing\n",
        "    theta_x = Conv2D(inter_shape, kernel_size = 1, strides = 1, padding='same', kernel_initializer='he_normal', activation=None)(x)\n",
        "    theta_x = MaxPooling2D((2,2))(theta_x)\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "    # gating signal \"\"\n",
        "    phi_g = Conv2D(inter_shape, kernel_size = 1, strides = 1, padding='same', kernel_initializer='he_normal', activation=None)(gating)\n",
        "    shape_phi_g = K.int_shape(phi_g)\n",
        "\n",
        "    # Add components\n",
        "    concat_xg = add([phi_g, theta_x])\n",
        "    act_xg = Activation('relu')(concat_xg)\n",
        "\n",
        "    # Apply convolution\n",
        "    psi = Conv2D(1, kernel_size = 1, strides = 1, padding='same', kernel_initializer='he_normal', activation=None)(act_xg)\n",
        "\n",
        "    # Apply sigmoid activation\n",
        "    sigmoid_xg = Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "\n",
        "    # UpSample and resample to correct size\n",
        "    upsample_psi = UpSampling2D(interpolation='bilinear', size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)\n",
        "    upsample_psi = tf.broadcast_to(upsample_psi, shape=shape_x)\n",
        "    y = multiply([upsample_psi, x])\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "'''\n",
        "  Attention U-Net model\n",
        "'''\n",
        "\n",
        "def UNetAM(trained_weights = None, input_size = (512,512,3), drop_rate = 0.25, lr=0.0001, filter_base=16):\n",
        "\n",
        "    ## Can add pretrained weights by specifying 'trained_weights'\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(input_size, batch_size=1)\n",
        "\n",
        "    ## Contraction phase\n",
        "    conv = convBlock2(inputs, filter_base, 3)\n",
        "    #conv0 = Dropout(drop_rate)(conv0)\n",
        "\n",
        "    conv0 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv0 = convBlock2(conv0, 2 * filter_base, 3)\n",
        "\n",
        "    pool0 = MaxPooling2D(pool_size=(2, 2))(conv0)\n",
        "    conv1 = convBlock2(pool0, 4 * filter_base, 3)\n",
        "    #conv1 = Dropout(drop_rate)(conv1)\n",
        "\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = convBlock2(pool1, 8 * filter_base, 3)\n",
        "    #conv2 = Dropout(drop_rate)(conv2)\n",
        "\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = convBlock2(pool2, 16 * filter_base, 3)\n",
        "    #conv3 = Dropout(drop_rate)(conv3)\n",
        "\n",
        "    ## Expansion phase\n",
        "    up4 = (Conv2DTranspose(8 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv3))\n",
        "    merge4 = attention_block(conv2, conv3, 8 * filter_base, drop_rate) # Attention gate\n",
        "    conv4 = concatenate([up4, merge4])\n",
        "    conv4 = convBlock2(conv4, 8 * filter_base, 3)\n",
        "\n",
        "    up5 = (Conv2DTranspose(4 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv4))\n",
        "    merge5 = attention_block(conv1, conv4, 4 * filter_base, drop_rate) # Attention gate\n",
        "    conv5 = concatenate([up5, merge5])\n",
        "    conv5 = convBlock2(conv5, 4 * filter_base, 3)\n",
        "\n",
        "    up6 = (Conv2DTranspose(2 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv5))\n",
        "    merge6 = attention_block(conv0, conv5, 2 * filter_base, drop_rate) # Attention gate\n",
        "    conv6 = concatenate([up6, merge6])\n",
        "    conv6 = convBlock2(conv6, 2 * filter_base, 3)\n",
        "\n",
        "    up7 = (Conv2DTranspose(1 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv6))\n",
        "    merge7 = attention_block(conv, conv6, 1 * filter_base, drop_rate) # Attention gate\n",
        "    conv7 = concatenate([up7, merge7])\n",
        "    conv7 = concatenate([up7, conv])\n",
        "    conv7 = convBlock2(conv7, 1 * filter_base, 3)\n",
        "\n",
        "    ## Output layer\n",
        "    out = convBlock(conv7, 1, 1, act='sigmoid')\n",
        "\n",
        "    model = Model(inputs, out)\n",
        "\n",
        "    model.compile(optimizer = adam_v2.Adam(learning_rate = lr), loss = binary_crossentropy, metrics = ['accuracy', 'mse'])\n",
        "\n",
        "    if trained_weights != None:\n",
        "    \tmodel.load_weights(trained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2fZD22p5xsF"
      },
      "source": [
        "# Print model layers and number of parameters\n",
        "UNetAM().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdVOW6aVayiS"
      },
      "source": [
        "## ResNet50-SegNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe9w__nwa4sx"
      },
      "source": [
        "# Forked code from: https://github.com/ykamikawa/tf-keras-SegNet\n",
        "\n",
        "from keras.layers import Layer\n",
        "\n",
        "'''\n",
        "  Unpooling using max pooling indices\n",
        "'''\n",
        "\n",
        "class MaxPoolingWithArgmax2D(Layer):\n",
        "    def __init__(self, pool_size=(2, 2), strides=(2, 2), padding=\"same\", **kwargs):\n",
        "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        padding = 'same'\n",
        "        pool_size = (2,2)\n",
        "        strides = (2,2)\n",
        "        if K.backend() == \"tensorflow\":\n",
        "            ksize = [1, pool_size[0], pool_size[1], 1]\n",
        "            padding = padding.upper()\n",
        "            strides = [1, strides[0], strides[1], 1]\n",
        "            output, argmax = K.tf.nn.max_pool_with_argmax(\n",
        "                inputs, ksize=ksize, strides=strides, padding=padding\n",
        "            )\n",
        "        else:\n",
        "            errmsg = \"{} backend is not supported for layer {}\".format(\n",
        "                K.backend(), type(self).__name__\n",
        "            )\n",
        "            raise NotImplementedError(errmsg)\n",
        "        argmax = K.cast(argmax, K.floatx())\n",
        "        return [output, argmax]\n",
        "\n",
        "class MaxUnpooling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), **kwargs):\n",
        "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, inputs, output_shape=None):\n",
        "        updates, mask = inputs[0], inputs[1]\n",
        "        with tf.compat.v1.variable_scope(self.name):\n",
        "            mask = K.cast(mask, \"int32\")\n",
        "            input_shape = K.tf.shape(updates, out_type=\"int32\")\n",
        "            #  calculation new shape\n",
        "            if output_shape is None:\n",
        "                output_shape = (\n",
        "                    input_shape[0],\n",
        "                    input_shape[1] * self.size[0],\n",
        "                    input_shape[2] * self.size[1],\n",
        "                    input_shape[3],\n",
        "                )\n",
        "            self.output_shape1 = output_shape\n",
        "\n",
        "            # calculation indices for batch, height, width and feature maps\n",
        "            one_like_mask = K.ones_like(mask, dtype=\"int32\")\n",
        "            batch_shape = K.concatenate([[input_shape[0]], [1], [1], [1]], axis=0)\n",
        "            batch_range = K.reshape(\n",
        "                K.tf.range(output_shape[0], dtype=\"int32\"), shape=batch_shape\n",
        "            )\n",
        "            b = one_like_mask * batch_range\n",
        "            y = mask // (output_shape[2] * output_shape[3])\n",
        "            x = (mask // output_shape[3]) % output_shape[2]\n",
        "            feature_range = K.tf.range(output_shape[3], dtype=\"int32\")\n",
        "            f = one_like_mask * feature_range\n",
        "\n",
        "            # transpose indices & reshape update values to one dimension\n",
        "            updates_size = K.tf.size(updates)\n",
        "            indices = K.transpose(K.reshape(K.stack([b, y, x, f]), [4, updates_size]))\n",
        "            values = K.reshape(updates, [updates_size])\n",
        "            ret = K.tf.scatter_nd(indices, values, output_shape)\n",
        "            return ret\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        mask_shape = input_shape[1]\n",
        "        return (\n",
        "            mask_shape[0],\n",
        "            mask_shape[1] * self.size[0],\n",
        "            mask_shape[2] * self.size[1],\n",
        "            mask_shape[3],\n",
        "        )\n",
        "\n",
        "# Custom version of MaxUnpooling2D\n",
        "# Takes raw layer values and outputs values\n",
        "# Takes tf.nn.max_pool_with_argmax output as input\n",
        "def unpool_with_indices(pool, indices, out_size=2):\n",
        "  print(pool)\n",
        "  print(indices)\n",
        "  # Create empty array of appropriate size\n",
        "  shape = np.array(np.shape(pool))\n",
        "  shape = np.array((shape[0], out_size * shape[1], out_size * shape[2], shape[3]))\n",
        "  out = np.zeros(shape)\n",
        "\n",
        "  # Make upsample\n",
        "  inds = np.array(indices).flatten()\n",
        "  outs = np.array(pool).flatten()\n",
        "  for i in range(len(inds)):\n",
        "    blk = inds[i] // (shape[2] * shape[3]) # Find which block to place numbers in\n",
        "    ln  = inds[i] - (blk * shape[3] * shape[2]) # Find which line\n",
        "    ln2 = ln // (shape[3]) # Find line\n",
        "    pos = ln % (shape[3]) # Find position\n",
        "    #print(blk, ln2, pos)\n",
        "    out[0][blk][ln2][pos] = outs[i]\n",
        "\n",
        "\n",
        "  #print(out.shape)\n",
        "  return (out)\n",
        "\n",
        "# Own custom code\n",
        "'''\n",
        "  ResNet Contraction Phase Block\n",
        "'''\n",
        "\n",
        "def resnetConvDownBlock(x, filter, kernel, act='relu'):\n",
        "  # Convolutional Block for encoding phase\n",
        "  for i in range(3):\n",
        "    x = ZeroPadding2D((1,1))(x)\n",
        "    x = Conv2D(filters = filter, kernel_size = kernel, kernel_initializer = 'he_normal')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "'''\n",
        "  SegNet Expansion Phase Block\n",
        "'''\n",
        "def resnetConvUpBlock(x, skip_connection = None, filter = None, kernel = None, act='relu'):\n",
        "  # Convolutional block for decoding phase\n",
        "\n",
        "  out = x\n",
        "\n",
        "  # Unpooling\n",
        "  out = UpSampling2D((2,2))(out)\n",
        "\n",
        "  # Conv Block\n",
        "  for i in range(3):\n",
        "    out = ZeroPadding2D((1,1))(out)\n",
        "    out = Conv2D(filters = filter, kernel_size = kernel, kernel_initializer = 'he_normal')(out)\n",
        "    out = Activation('relu')(out)\n",
        "\n",
        "  # Implement skip connection\n",
        "  if skip_connection != None:\n",
        "    out = Add()([out, skip_connection])\n",
        "\n",
        "  return out\n",
        "\n",
        "def ResNet50SegNet(input_size=(512,512,3), lr = 0.0001, filters = 64, kernel_sz = 3):\n",
        "\n",
        "  inputs = Input(input_size)\n",
        "\n",
        "  # Encoder\n",
        "  # Conv, Conv, Conv, MaxPool #1\n",
        "  block1 = resnetConvDownBlock(inputs, filter = filters, kernel = kernel_sz)\n",
        "  pool1, mask1 = MaxPoolingWithArgmax2D((2,2))(block1)\n",
        "  # Conv, Conv, Conv, MaxPool #2\n",
        "  block2 = resnetConvDownBlock(pool1, filter = 2 * filters, kernel = kernel_sz)\n",
        "  pool2, mask2 = MaxPoolingWithArgmax2D((2,2))(block2)\n",
        "  # Conv, Conv, Conv, MaxPool #3\n",
        "  block3 = resnetConvDownBlock(pool2, filter = 4 * filters, kernel = kernel_sz)\n",
        "  pool3, mask3 = MaxPoolingWithArgmax2D((2,2))(block3)\n",
        "  # Conv, Conv, Conv, MaxPool #4\n",
        "  block4 = resnetConvDownBlock(pool3, filter = 8 * filters, kernel = kernel_sz)\n",
        "  pool4, mask4 = MaxPoolingWithArgmax2D((2,2))(block4)\n",
        "  # Conv, Conv, Conv, MaxPool #5\n",
        "  block5 = resnetConvDownBlock(pool4, filter = 16 * filters, kernel = kernel_sz)\n",
        "  pool5, mask5 = MaxPoolingWithArgmax2D((2,2))(block5)\n",
        "\n",
        "  # Decoder\n",
        "  # ConvTranspose + Concat, Conv, Conv, Conv #1\n",
        "  block5_ = resnetConvUpBlock(pool5, filter = 16 * filters, kernel = kernel_sz)\n",
        "  # ConvTranspose + Concat, Conv, Conv, Conv #2\n",
        "  block4_ = resnetConvUpBlock(block5_, skip_connection = MaxUnpooling2D((2,2))([pool4, mask4]), filter = 8 * filters, kernel = kernel_sz)\n",
        "  # ConvTranspose + Concat, Conv, Conv, Conv #3\n",
        "  block3_ = resnetConvUpBlock(block4_, skip_connection = MaxUnpooling2D((2,2))([pool3, mask3]), filter = 4 * filters, kernel = kernel_sz)\n",
        "  # ConvTranspose + Concat, Conv, Conv, Conv #4\n",
        "  block2_ = resnetConvUpBlock(block3_, skip_connection = MaxUnpooling2D((2,2))([pool2, mask2]), filter = 2 * filters, kernel = kernel_sz)\n",
        "  # ConvTranspose + Concat, Conv, Conv, Conv #5\n",
        "  block1_ = resnetConvUpBlock(block2_, skip_connection = MaxUnpooling2D((2,2))([pool1, mask1]), filter = filters, kernel = kernel_sz)\n",
        "\n",
        "  # Output\n",
        "  outputs = Conv2D(1, kernel_size = 1, strides = 1, kernel_initializer = 'he_normal')(block1_)\n",
        "  outputs = Activation('sigmoid')(outputs)\n",
        "\n",
        "  model = Model(inputs, outputs)\n",
        "  model.compile(optimizer = adam_v2.Adam(learning_rate = lr), loss = binary_crossentropy, metrics = ['accuracy', 'mse'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOwTLb7-a8il"
      },
      "source": [
        "# Print model layers and number of parameters\n",
        "ResNet50SegNet().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqSGcX06a2Vw"
      },
      "source": [
        "## FCN32-VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "436NycoGa4Qa"
      },
      "source": [
        "# Code forked and modified from: https://github.com/divamgupta/image-segmentation-keras\n",
        "\n",
        "'''\n",
        "  FCN32-VGG16 model\n",
        "'''\n",
        "\n",
        "def fcn_32(input_size = (512,512,3), lr = 0.0001, drop_rate = 0):\n",
        "\n",
        "    kernel = 3\n",
        "    filter_size = 64\n",
        "    pad = 1\n",
        "    pool_size = 2\n",
        "\n",
        "    IMAGE_ORDERING = 'channels_last'\n",
        "    # Input\n",
        "    inputs = Input(shape=input_size)\n",
        "\n",
        "    x = inputs\n",
        "    levels = []\n",
        "\n",
        "    ## Encoder\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), padding='same',\n",
        "               name='block1_conv1', data_format=IMAGE_ORDERING)(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same',\n",
        "               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    levels.append(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), padding='same',\n",
        "               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(128, (3, 3), padding='same',\n",
        "               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    levels.append(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), padding='same',\n",
        "               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (3, 3), padding='same',\n",
        "               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (3, 3), padding='same',\n",
        "               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    levels.append(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), padding='same',\n",
        "               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(512, (3, 3), padding='same',\n",
        "               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(512, (3, 3), padding='same',\n",
        "               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    levels.append(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), padding='same',\n",
        "               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(512, (3, 3), padding='same',\n",
        "               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(512, (3, 3), padding='same',\n",
        "               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "\n",
        "    levels.append(x)\n",
        "\n",
        "    [f1, f2, f3, f4, f5] = levels\n",
        "\n",
        "    o = f5\n",
        "\n",
        "    # Decoder\n",
        "    o = (Conv2D(4096, (7 , 7 ), padding = 'same', kernel_initializer = 'he_normal', name = \"conv6\"))(o)\n",
        "    o = Activation('relu')(o)\n",
        "    o = Dropout(drop_rate)(o)\n",
        "    o = (Conv2D(4096, (1 , 1 ), padding = 'same', kernel_initializer = 'he_normal', name = \"conv7\"))(o)\n",
        "    o = Activation('relu')(o)\n",
        "    o = Dropout(drop_rate)(o)\n",
        "\n",
        "    o = (Conv2D(1, 1, padding='same', kernel_initializer='he_normal', name=\"scorer1\"))(o)\n",
        "    o = Conv2DTranspose(1, kernel_size=(64,64), padding='same', strides=(32,32), name=\"Upsample32\")(o)\n",
        "    o = (Conv2D(1, 1, padding='same', kernel_initializer='he_normal', name=\"output\"))(o)\n",
        "\n",
        "    # Output\n",
        "    o = Activation('sigmoid')(o)\n",
        "\n",
        "    model = Model(inputs, o)\n",
        "    model.compile(optimizer = adam_v2.Adam(learning_rate = lr), loss = binary_crossentropy, metrics = ['accuracy', 'mse'])\n",
        "    model.model_name = \"fcn_32\"\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abdIZUEMa_zx"
      },
      "source": [
        "# Print model layers and number of parameters\n",
        "fcn_32().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIqC7Ho3ZXj4"
      },
      "source": [
        "## ResUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3KpHg5VZax-"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def res_block_initial(x, num_filters, kernel_size, strides, name):\n",
        "    \"\"\"Residual Unet block layer for first layer\n",
        "    In the residual unet the first residual block does not contain an\n",
        "    initial batch normalization and activation so we create this separate\n",
        "    block for it.\n",
        "    Args:\n",
        "        x: tensor, image or image activation\n",
        "        num_filters: list, contains the number of filters for each subblock\n",
        "        kernel_size: int, size of the convolutional kernel\n",
        "        strides: list, contains the stride for each subblock convolution\n",
        "        name: name of the layer\n",
        "    Returns:\n",
        "        x1: tensor, output from residual connection of x and x1\n",
        "    \"\"\"\n",
        "\n",
        "    if len(num_filters) == 1:\n",
        "        num_filters = [num_filters[0], num_filters[0]]\n",
        "\n",
        "    x1 = tf.keras.layers.Conv2D(filters=num_filters[0],\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides[0],\n",
        "                                padding='same',\n",
        "                                kernel_initializer = 'he_normal',\n",
        "                                name=name+'_1')(x)\n",
        "    #x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "    x1 = tf.keras.layers.Conv2D(filters=num_filters[1],\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides[1],\n",
        "                                padding='same',\n",
        "                                kernel_initializer = 'he_normal',\n",
        "                                name=name+'_2')(x1)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=num_filters[-1],\n",
        "                                kernel_size=1,\n",
        "                                strides=1,\n",
        "                                padding='same',\n",
        "                                kernel_initializer = 'he_normal',\n",
        "                                name=name+'_shortcut')(x)\n",
        "    #x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Add()([x, x1])\n",
        "\n",
        "    return x1\n",
        "\n",
        "def res_block(x, num_filters, kernel_size, strides, name):\n",
        "    \"\"\"Residual Unet block layer\n",
        "    Consists of batch norm and relu, folowed by conv, batch norm and relu and\n",
        "    final convolution. The input is then put through\n",
        "    Args:\n",
        "        x: tensor, image or image activation\n",
        "        num_filters: list, contains the number of filters for each subblock\n",
        "        kernel_size: int, size of the convolutional kernel\n",
        "        strides: list, contains the stride for each subblock convolution\n",
        "        name: name of the layer\n",
        "    Returns:\n",
        "        x1: tensor, output from residual connection of x and x1\n",
        "    \"\"\"\n",
        "\n",
        "    if len(num_filters) == 1:\n",
        "        num_filters = [num_filters[0], num_filters[0]]\n",
        "\n",
        "    #x1 = tf.keras.layers.BatchNormalization()(x)\n",
        "    #x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "    x1 = tf.keras.layers.Activation('relu')(x)\n",
        "    x1 = tf.keras.layers.Conv2D(filters=num_filters[0],\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides[0],\n",
        "                                padding='same',\n",
        "                                kernel_initializer = 'he_normal',\n",
        "                                name=name+'_1')(x1)\n",
        "    #x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "    x1 = tf.keras.layers.Conv2D(filters=num_filters[1],\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides[1],\n",
        "                                padding='same',\n",
        "                                kernel_initializer = 'he_normal',\n",
        "                                name=name+'_2')(x1)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=num_filters[-1],\n",
        "                                    kernel_size=1,\n",
        "                                    strides=strides[0],\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer = 'he_normal',\n",
        "                                    name=name+'_shortcut')(x)\n",
        "    #x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Add()([x, x1])\n",
        "\n",
        "    return x1\n",
        "\n",
        "\n",
        "def upsample(x, target_size):\n",
        "    \"\"\"\"Upsampling function, upsamples the feature map\n",
        "    Deep Residual Unet paper does not describe the upsampling function\n",
        "    in detail. Original Unet uses a transpose convolution that downsamples\n",
        "    the number of feature maps. In order to restrict the number of\n",
        "    parameters here we use a bilinear resampling layer. This results in\n",
        "    the concatentation layer concatenting feature maps with n and n/2\n",
        "    features as opposed to n/2  and n/2 in the original unet.\n",
        "    Args:\n",
        "        x: tensor, feature map\n",
        "        target_size: size to resize feature map to\n",
        "    Returns:\n",
        "        x_resized: tensor, upsampled feature map\n",
        "    \"\"\"\n",
        "\n",
        "    x_resized = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, target_size))(x)\n",
        "\n",
        "    return x_resized\n",
        "\n",
        "def encoder(x, num_filters, kernel_size):\n",
        "    \"\"\"Unet encoder\n",
        "    Args:\n",
        "        x: tensor, output from previous layer\n",
        "        num_filters: list, number of filters for each decoder layer\n",
        "        kernel_size: int, size of the convolutional kernel\n",
        "    Returns:\n",
        "        encoder_output: list, output from all encoder layers\n",
        "    \"\"\"\n",
        "\n",
        "    x = res_block_initial(x, [num_filters[0]], kernel_size, strides=[1,1], name='layer1')\n",
        "\n",
        "    encoder_output = [x]\n",
        "    for i in range(1, len(num_filters)):\n",
        "        layer = 'encoder_layer' + str(i)\n",
        "        x = res_block(x, [num_filters[i]], kernel_size, strides=[2,1], name=layer)\n",
        "        encoder_output.append(x)\n",
        "\n",
        "    return encoder_output\n",
        "\n",
        "def decoder(x, encoder_output, num_filters, kernel_size):\n",
        "    \"\"\"Unet decoder\n",
        "    Args:\n",
        "        x: tensor, output from previous layer\n",
        "        encoder_output: list, output from all previous encoder layers\n",
        "        num_filters: list, number of filters for each decoder layer\n",
        "        kernel_size: int, size of the convolutional kernel\n",
        "    Returns:\n",
        "        x: tensor, output from last layer of decoder\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(1, len(num_filters) + 1):\n",
        "        layer = 'decoder_layer' + str(i)\n",
        "        target_size = encoder_output[-i].shape[1:3]\n",
        "        x = upsample(x, target_size)\n",
        "        x = tf.keras.layers.Concatenate(axis=-1)([x, encoder_output[-i]])\n",
        "        x = res_block(x, [num_filters[-i]], kernel_size, strides=[1,1], name=layer)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def res_unet(input_size, num_filters, kernel_size, num_channels, num_classes, lr = 0.0001, out_layer = 'sigmoid', lossfunc='binary_crossentropy', num_out=1):\n",
        "    \"\"\"Residual Unet\n",
        "    Function that generates a residual unet\n",
        "    Args:\n",
        "        input_size: int, dimension of the input image\n",
        "        num_layers: int, number of layers in the encoder half, excludes bridge\n",
        "        num_filters: list, number of filters for each encoder layer\n",
        "        kernel_size: size of the kernel, applied to all convolutions\n",
        "        num_channels: int, number of channels for the input image\n",
        "        num_classes: int, number of output classes for the output\n",
        "    Returns:\n",
        "        model: tensorflow keras model for residual unet architecture\n",
        "    \"\"\"\n",
        "\n",
        "    x = tf.keras.Input(shape=[input_size, input_size, num_channels])\n",
        "\n",
        "    encoder_output = encoder(x, num_filters, kernel_size)\n",
        "\n",
        "    # bridge layer, number of filters is double that of the last encoder layer\n",
        "    bridge = res_block(encoder_output[-1], [num_filters[-1]*2], kernel_size,\n",
        "                        strides=[2,1], name='bridge')\n",
        "\n",
        "    decoder_output = decoder(bridge, encoder_output, num_filters, kernel_size)\n",
        "\n",
        "    output = tf.keras.layers.Conv2D(num_classes,\n",
        "                                    kernel_size,\n",
        "                                    strides=1,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer = 'he_normal',\n",
        "                                    name='output')(decoder_output)\n",
        "\n",
        "    output = Activation(out_layer)(output)\n",
        "\n",
        "    model = tf.keras.Model(x, output)\n",
        "    model.compile(optimizer = adam_v2.Adam(learning_rate = lr), loss = lossfunc, metrics=['accuracy', 'mse'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hamH2uwZp4R"
      },
      "source": [
        "res_unet(512, [64, 128, 256, 512], 3, 3, 1).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqNWFF1wyZtE"
      },
      "source": [
        "# Train on RGB feature data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbgqIFEvty2"
      },
      "source": [
        "## U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsAnxDvH1aIj"
      },
      "source": [
        "# Train U-Net with generator\n",
        "model_unet = UNet(input_size=(512,512,3), lr=0.0001)\n",
        "save_model = ModelCheckpoint('unet-3d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "train = trainGenerator(1, t_images, t_masks, data_gen_args, save_to_dir=None)\n",
        "\n",
        "model_unet.fit(train, steps_per_epoch=100, epochs=30, validation_data = validation_df, callbacks=[save_model])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SCg0Km09uZ_"
      },
      "source": [
        "# Save model training history\n",
        "np.save('unet-3d-history.npy',model_unet.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF9yrB7D4GXj"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-3d.hdf5 drive/MyDrive/Diss/\n",
        "!cp unet-3d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC25E6Bkmf9j"
      },
      "source": [
        "# Plot accuracy and loss\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(model_unet.history.history['accuracy'])\n",
        "plt.plot(model_unet.history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(model_unet.history.history['loss'])\n",
        "plt.plot(model_unet.history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrXjUZiI5oNH"
      },
      "source": [
        "## Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOeeBtkaXU03"
      },
      "source": [
        "# Train Attention U-Net with generator\n",
        "model_attention_unet = UNetAM(lr=0.0005, filter_base=16)\n",
        "save_model_am = ModelCheckpoint('unet-attention-3d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "train = trainGenerator(1, t_images, t_masks, data_gen_args, save_to_dir=None)\n",
        "model_attention_unet.fit(train, steps_per_epoch=100, epochs=50, validation_data = validation_df, callbacks=[save_model_am])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azsRDL0eBMy7"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-attention-3d-history.npy',model_attention_unet.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOjdBOjMucRJ"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-attention-3d.hdf5 drive/MyDrive/Diss/\n",
        "!cp unet-attention-3d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ysgFtEb56u"
      },
      "source": [
        "## ResNet50-SegNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yqXR3Iie0jD"
      },
      "source": [
        "R = ResNet50SegNet()\n",
        "save_model_resnet = ModelCheckpoint('resnet50segnet-3d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, save_weights_only=True)\n",
        "train = trainGenerator(1, t_images, t_masks, data_gen_args, save_to_dir=None)\n",
        "R.fit(train, validation_data = validation_df, epochs = 40, steps_per_epoch = 100, callbacks=[save_model_resnet])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVyJOLvE7jz4"
      },
      "source": [
        "# Save model history\n",
        "np.save('resnet50segnet-3d-history.npy',R.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s--M2ygStGQi"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp resnet50segnet-3d.hdf5 drive/MyDrive/Diss/models/\n",
        "!cp resnet50segnet-3d-history.npy drive/MyDrive/Diss/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttReyVJlb-yC"
      },
      "source": [
        "## FCN32-VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45I0hjSxm6An"
      },
      "source": [
        "F = fcn_32(lr = 0.0001)\n",
        "save_model_fcn32 = ModelCheckpoint('fcn32-3d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "train = trainGenerator(1, t_images, t_masks, data_gen_args, save_to_dir=None)\n",
        "F.fit(train, validation_data = validation_df, epochs=50, steps_per_epoch = 100, shuffle = True, callbacks=[save_model_fcn32])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KKMa6HBFK0i"
      },
      "source": [
        "# Save model history\n",
        "np.save('fcn32-3d-history.npy', F.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu9fumpOqB4R"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp fcn32-3d.hdf5 drive/MyDrive/Diss/\n",
        "!cp fcn32-3d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkaV2Qi8d-VC"
      },
      "source": [
        "## ResUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN5sloqgd-VF"
      },
      "source": [
        "R = res_unet(512, [64, 128, 256, 512], 3, 3, 1)\n",
        "save_model_resunet = ModelCheckpoint('resunet-3d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, save_weights_only=True)\n",
        "train = trainGenerator(1, t_images, t_masks, data_gen_args, save_to_dir=None)\n",
        "R.fit(train, validation_data = validation_df, epochs = 40, steps_per_epoch = 100, callbacks=[save_model_resunet])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH9RhGmLd-VJ"
      },
      "source": [
        "# Save model history\n",
        "np.save('resunet-3d-history.npy',R.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBizNdxcd-VL"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp resunet-3d.hdf5 drive/MyDrive/Diss/models/\n",
        "!cp resunet-3d-history.npy drive/MyDrive/Diss/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5KsYCx34JdT"
      },
      "source": [
        "# Train on 4-band data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdPUGj0iCqX9"
      },
      "source": [
        "## Train on 4-band Amazon data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZXjnnfiFU6"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMXibzYiCtV6"
      },
      "source": [
        "# Train U-Net with generator\n",
        "model_unet_4band = UNet()\n",
        "save_model_4band = ModelCheckpoint('unet-4d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "model_unet_4band.fit(train_df_4band_amazon, epochs = 20, validation_data = validation_df_4band_amazon, callbacks=[save_model_4band])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Sf1pmCDI-c"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-4d-history.npy',model_unet_4band.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM86ZMtryOi2"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-4d.hdf5 drive/MyDrive/Diss/\n",
        "!cp unet-4d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HICnX_kEQDhR"
      },
      "source": [
        "### Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7bblQxyoyZd"
      },
      "source": [
        "# Train U-Net with generator\n",
        "model_attention_unet_4band = UNetAM(input_size=(512,512,4), filter_base=16, lr=0.0005)\n",
        "save_model_4band_attention = ModelCheckpoint('unet-attention-4d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "model_attention_unet_4band.fit(train_df_4band_amazon, epochs = 60, validation_data = validation_df_4band_amazon, callbacks=[save_model_4band_attention])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU7s-wvYQgy7"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-attention-4d-history.npy',model_attention_unet_4band.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8jeWN9XfEO-"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-attention-4d.hdf5 drive/MyDrive/Diss/\n",
        "!cp unet-attention-4d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WUlSrYEckkL"
      },
      "source": [
        "### ResNet50-SegNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeadji76_dU"
      },
      "source": [
        "R_4band = ResNet50SegNet(input_size=(512,512,4))\n",
        "save_model_resnet_4band = ModelCheckpoint('resnet50segnet-4d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, save_weights_only=True)\n",
        "R_4band.fit(train_df_4band_amazon, validation_data = validation_df_4band_amazon, epochs = 20, callbacks=[save_model_resnet_4band])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LInS5I9M7n2O"
      },
      "source": [
        "# Save model history\n",
        "np.save('resnet50segnet-4d-history.npy', R_4band.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA1hQvbz90PR"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp resnet50segnet-4d.hdf5 drive/MyDrive/Diss/models/\n",
        "!cp resnet50segnet-4d-history.npy drive/MyDrive/Diss/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj53J2Bicnk0"
      },
      "source": [
        "### FCN32-VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBnvanCf3dom"
      },
      "source": [
        "F_4band = fcn_32(input_size=(512,512,4), lr = 0.0001)\n",
        "save_model_fcn_4band = ModelCheckpoint('fcn32-4d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "F_4band.fit(train_df_4band_amazon, validation_data = validation_df_4band_amazon, epochs = 50, callbacks=[save_model_fcn_4band])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VH3Ic51Nswz"
      },
      "source": [
        "# Save model history\n",
        "np.save('fcn32-4d-history.npy', F_4band.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAfV82jL5nOv"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp fcn32-4d.hdf5 drive/MyDrive/Diss/\n",
        "!cp fcn32-4d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySQWsRnxf1ZW"
      },
      "source": [
        "### ResUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17XuKZHuf1Za"
      },
      "source": [
        "R_4band = res_unet(512, [64, 128, 256, 512], 3, 4, 1)\n",
        "save_model_resunet_4band = ModelCheckpoint('resunet-4d.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, save_weights_only=True)\n",
        "R_4band.fit(train_df_4band_amazon, validation_data = validation_df_4band_amazon, epochs = 20, callbacks=[save_model_resunet_4band])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM_HotYbf1Zd"
      },
      "source": [
        "# Save model history\n",
        "np.save('resunet-4d-history.npy', R_4band.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVt-w-nf1Zg"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp resunet-4d.hdf5 drive/MyDrive/Diss/models/\n",
        "!cp resunet-4d-history.npy drive/MyDrive/Diss/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKl_IU1q4yeE"
      },
      "source": [
        "## Train on 4-band Atlantic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDa_hakb42II"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULAHseJg403h"
      },
      "source": [
        "# Train U-Net with generator\n",
        "model_unet_4band_atlantic = UNet()\n",
        "save_model_4band_atlantic = ModelCheckpoint('unet-4d-atlantic.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "model_unet_4band_atlantic.fit(train_df_4band_atlantic, epochs = 20, validation_data = validation_df_4band_atlantic, callbacks=[save_model_4band_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTWiRbk5NLN"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-4d-atlantic-history.npy',model_unet_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIQWXYMoNi7"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-4d-atlantic.hdf5 drive/MyDrive/Diss/\n",
        "!cp unet-4d-atlantic-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieuJPBse44cA"
      },
      "source": [
        "### Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEvQIolulSNp"
      },
      "source": [
        "# Train Attention U-Net with generator\n",
        "model_attention_unet_4band_atlantic = UNetAM(input_size=(512,512,4), filter_base=16, lr=0.0005)\n",
        "save_model_4band_attention_atlantic = ModelCheckpoint('unet-attention-4d-atlantic.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "model_attention_unet_4band_atlantic.fit(train_df_4band_atlantic, epochs = 60, validation_data = validation_df_4band_atlantic, callbacks=[save_model_4band_attention_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vYhP4_25caq"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-attention-4d-atlantic-history.npy',model_attention_unet_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMCaWw6On6_D"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-attention-4d-atlantic.hdf5 drive/MyDrive/Diss/\n",
        "!cp unet-attention-4d-atlantic-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd8XRreacw67"
      },
      "source": [
        "### ResNet50-SegNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyibZJTeno4p"
      },
      "source": [
        "R_4band_atlantic = ResNet50SegNet(input_size=(512,512,4))\n",
        "save_model_resnet_4band_atlantic = ModelCheckpoint('resnet50segnet-4d-atlantic.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, save_weights_only=True)\n",
        "R_4band_atlantic.fit(train_df_4band_atlantic, validation_data = validation_df_4band_atlantic, epochs = 30, callbacks=[save_model_resnet_4band_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAo4JFJS7s88"
      },
      "source": [
        "# Save model history\n",
        "np.save('resnet50segnet-4d-history-atlantic.npy', R_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfnpKigS95_2"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp resnet50segnet-4d-atlantic.hdf5 drive/MyDrive/Diss/models/\n",
        "!cp resnet50segnet-4d-history-atlantic.npy drive/MyDrive/Diss/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fAgIwTNc2oj"
      },
      "source": [
        "### FCN32-VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA5Do4LK3eSR"
      },
      "source": [
        "F_4band_atlantic = fcn_32(input_size=(512,512,4), lr = 0.0001)\n",
        "save_model_fcn32_4band_atlantic = ModelCheckpoint('fcn32-4d-atlantic.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "F_4band_atlantic.fit(train_df_4band_atlantic, validation_data = validation_df_4band_atlantic, epochs = 50, callbacks=[save_model_fcn32_4band_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKGjlS7XNvcT"
      },
      "source": [
        "# Save model history\n",
        "np.save('fcn32-4d-atlantic-history.npy', F_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5On0tO65pkc"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp fcn32-4d-atlantic.hdf5 drive/MyDrive/Diss/\n",
        "!cp fcn32-4d-atlantic-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsY7QhpgLBA"
      },
      "source": [
        "### ResUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpikJDv9gLBD"
      },
      "source": [
        "R_4band_atlantic = res_unet(512, [64, 128, 256, 512], 3, 4, 1)\n",
        "save_model_resunet_4band_atlantic = ModelCheckpoint('resunet-4d-atlantic.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, save_weights_only=True)\n",
        "R_4band_atlantic.fit(train_df_4band_atlantic, validation_data = validation_df_4band_atlantic, epochs = 30, callbacks=[save_model_resunet_4band_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sezbJ7EPgLBF"
      },
      "source": [
        "# Save model history\n",
        "np.save('resunet-4d-history-atlantic.npy', R_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtJXLnjmgLBH"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp resunet-4d-atlantic.hdf5 drive/MyDrive/Diss/models/\n",
        "!cp resunet-4d-history-atlantic.npy drive/MyDrive/Diss/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JXc78rQLaL"
      },
      "source": [
        "# Import Models and Compute Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPCKtyBZej-"
      },
      "source": [
        "## RGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6EorDk5QLaM"
      },
      "source": [
        "# Load 3-dim models and history stats\n",
        "attention_unet = load_model('unet-attention-3d.hdf5')\n",
        "unet = load_model('unet-3d.hdf5')\n",
        "\n",
        "unet_history = np.load('unet-3d-history.npy', allow_pickle='TRUE').item()\n",
        "attention_unet_history = np.load('unet-attention-3d-history.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwm2nd6mAJqa"
      },
      "source": [
        "# Plot accuracy and loss for U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(unet_history['accuracy'])\n",
        "plt.plot(unet_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(unet_history['loss'])\n",
        "plt.plot(unet_history['val_loss'])\n",
        "plt.ylabel('Loss', size=12)\n",
        "plt.xlabel('Epoch', size=12)\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTNTxObMBnEZ"
      },
      "source": [
        "# Plot accuracy and loss for Attention U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(attention_unet_history['accuracy'])\n",
        "plt.plot(attention_unet_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(attention_unet_history['loss'])\n",
        "plt.plot(attention_unet_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL_aqt5rQLaM"
      },
      "source": [
        "# Scores of each model\n",
        "unet_score = (score_eval(unet, validation_images, validation_masks))\n",
        "am_unet_score = (score_eval(attention_unet, validation_images, validation_masks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn_a9omPQLaM"
      },
      "source": [
        "# Precision and recall of each model\n",
        "unet_precision = (precision_eval(unet, validation_images, validation_masks))\n",
        "am_unet_precision = (precision_eval(attention_unet, validation_images, validation_masks))\n",
        "\n",
        "unet_recall = (recall_eval(unet, validation_images, validation_masks))\n",
        "am_unet_recall = (recall_eval(attention_unet, validation_images, validation_masks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj5HwfA1QLaM"
      },
      "source": [
        "# F1-scores of each model\n",
        "unet_f1_score = (f1_score_eval_basic(unet_precision, unet_recall))\n",
        "am_unet_f1_score = (f1_score_eval_basic(am_unet_precision, am_unet_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr93PcumQLaM"
      },
      "source": [
        "# Print score eval results for each model\n",
        "print('U-Net accuracy: ', np.mean(unet_score), np.std(unet_score))\n",
        "print('Attention U-Net accuracy: ', np.mean(am_unet_score), np.std(am_unet_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1021oSlAQLaN"
      },
      "source": [
        "# Print precision eval results for each model\n",
        "print('U-Net precision: ', np.mean(unet_precision), np.std(unet_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_precision), np.std(am_unet_precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRGd9iNlQLaN"
      },
      "source": [
        "# Print recall eval results for each model\n",
        "print('U-Net recall: ', np.mean(unet_recall), np.std(unet_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_recall), np.std(am_unet_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFzHHEknQLaN"
      },
      "source": [
        "# Print f1-score eval results for each model\n",
        "print('U-Net F1-score: ', np.mean(unet_f1_score))\n",
        "print('Attention U-Net F1-score: ', np.mean(am_unet_f1_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JVLAuzIZhWv"
      },
      "source": [
        "## 4-band"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI5RTFiZD9W-"
      },
      "source": [
        "### Amazon Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuWpNDuLZS2W"
      },
      "source": [
        "# Load 4-dim models and history stats\n",
        "attention_unet_4d = load_model('unet-attention-4d.hdf5')\n",
        "unet_4d = load_model('unet-4d.hdf5')\n",
        "\n",
        "unet_4d_history = np.load('unet-4d-history.npy', allow_pickle='TRUE').item()\n",
        "attention_unet_4d_history = np.load('unet-attention-4d-history.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwDcH3DJQLaO"
      },
      "source": [
        "# Plot accuracy and loss for U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(unet_4d_history['accuracy'])\n",
        "plt.plot(unet_4d_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(unet_4d_history['loss'])\n",
        "plt.plot(unet_4d_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mcn0YjbZpPL"
      },
      "source": [
        "# Plot accuracy and loss for Attention U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(attention_unet_4d_history['accuracy'])\n",
        "plt.plot(attention_unet_4d_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(attention_unet_4d_history['loss'])\n",
        "plt.plot(attention_unet_4d_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1V4qoM4ZtjE"
      },
      "source": [
        "# Scores of each model\n",
        "unet_4d_score = (score_eval2(unet_4d, validation_images2, validation_masks2))\n",
        "am_unet_4d_score = (score_eval2(attention_unet_4d, validation_images2, validation_masks2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVcI_yk4Z80x"
      },
      "source": [
        "# Precision and recall of each model\n",
        "unet_4d_precision = (precision_eval(unet_4d, validation_images2, validation_masks2))\n",
        "am_unet_4d_precision = (precision_eval(attention_unet_4d, validation_images2, validation_masks2))\n",
        "\n",
        "unet_4d_recall = (recall_eval(unet_4d, validation_images2, validation_masks2))\n",
        "am_unet_4d_recall = (recall_eval(attention_unet_4d, validation_images2, validation_masks2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvarhjO1aTX_"
      },
      "source": [
        "# F1-scores of each model\n",
        "unet_4d_f1_score = (f1_score_eval_basic(unet_4d_precision, unet_4d_recall))\n",
        "am_unet_4d_f1_score = (f1_score_eval_basic(am_unet_4d_precision, am_unet_4d_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htyl7Pi-aU88"
      },
      "source": [
        "# Print score eval results for each model\n",
        "print('U-Net accuracy: ', np.mean(unet_4d_score), np.std(unet_4d_score))\n",
        "print('Attention U-Net accuracy: ', np.mean(am_unet_4d_score), np.std(am_unet_4d_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geaOxWrpaWi9"
      },
      "source": [
        "# Print precision eval results for each model\n",
        "print('U-Net precision: ', np.mean(unet_4d_precision), np.std(unet_4d_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_precision), np.std(am_unet_4d_precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mpmu_7EaYEH"
      },
      "source": [
        "# Print recall eval results for each model\n",
        "print('U-Net recall: ', np.mean(unet_4d_recall), np.std(unet_4d_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_recall), np.std(am_unet_4d_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0NXQpR2aYZG"
      },
      "source": [
        "# Print f1-score eval results for each model\n",
        "print('U-Net F1-score: ', np.mean(unet_4d_f1_score))\n",
        "print('Attention U-Net F1-score: ', np.mean(am_unet_4d_f1_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beZYrcZdzE0c"
      },
      "source": [
        "### Amazon on unseen Atlantic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7LTlQvAzH8K"
      },
      "source": [
        "# Score\n",
        "unet_amazon_on_atlantic_score = score_eval2(unet_4d, validation_images3+test_images3, validation_masks3+test_masks3)\n",
        "am_unet_amazon_on_atlantic_score = score_eval2(attention_unet_4d, validation_images3+test_images3, validation_masks3+test_masks3)\n",
        "\n",
        "# Precision\n",
        "unet_amazon_on_atlantic_precision = (precision_eval(unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "am_unet_amazon_on_atlantic_precision = (precision_eval(attention_unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "\n",
        "# Recall\n",
        "unet_amazon_on_atlantic_recall = (recall_eval(unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "am_unet_amazon_on_atlantic_recall = (recall_eval(attention_unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_amazon_on_atlantic_f1_score = (f1_score_eval_basic(unet_amazon_on_atlantic_precision, unet_amazon_on_atlantic_recall))\n",
        "am_unet_amazon_on_atlantic_f1_score = (f1_score_eval_basic(am_unet_amazon_on_atlantic_precision, am_unet_amazon_on_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtAbkWSY0Xnn"
      },
      "source": [
        "# Print metrics\n",
        "print('U-Net score: ', np.mean(unet_amazon_on_atlantic_score), np.std(unet_amazon_on_atlantic_score))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_amazon_on_atlantic_score), np.std(am_unet_amazon_on_atlantic_score))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_amazon_on_atlantic_precision), np.std(unet_amazon_on_atlantic_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_amazon_on_atlantic_precision), np.std(am_unet_amazon_on_atlantic_precision))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_amazon_on_atlantic_recall), np.std(unet_amazon_on_atlantic_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_amazon_on_atlantic_recall), np.std(am_unet_amazon_on_atlantic_recall))\n",
        "\n",
        "print('U-Net F1-score: ', unet_amazon_on_atlantic_f1_score)\n",
        "print('Attention U-Net F1-score: ', am_unet_amazon_on_atlantic_f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRQOG9uUECcu"
      },
      "source": [
        "### Atlantic Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQIc0XDmEFND"
      },
      "source": [
        "# Load 4-dim models and history stats\n",
        "attention_unet_4d_atlantic = load_model('unet-attention-4d-atlantic.hdf5')\n",
        "unet_4d_atlantic = load_model('unet-4d-atlantic.hdf5')\n",
        "\n",
        "unet_4d_atlantic_history = np.load('unet-4d-atlantic-history.npy', allow_pickle='TRUE').item()\n",
        "attention_unet_4d_atlantic_history = np.load('unet-attention-4d-atlantic-history.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4YBSQxjEWLc"
      },
      "source": [
        "# Plot accuracy and loss for U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(unet_4d_atlantic_history['accuracy'])\n",
        "plt.plot(unet_4d_atlantic_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(unet_4d_atlantic_history['loss'])\n",
        "plt.plot(unet_4d_atlantic_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCFSJmMzEaR5"
      },
      "source": [
        "# Plot accuracy and loss for Attention U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(attention_unet_4d_atlantic_history['accuracy'])\n",
        "plt.plot(attention_unet_4d_atlantic_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(attention_unet_4d_atlantic_history['loss'])\n",
        "plt.plot(attention_unet_4d_atlantic_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNxllVnXEhvb"
      },
      "source": [
        "# Scores of each model\n",
        "unet_4d_atlantic_score = (score_eval2(unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "am_unet_4d_atlantic_score = (score_eval2(attention_unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "unet_4d_atlantic_precision = (precision_eval(unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "am_unet_4d_atlantic_precision = (precision_eval(attention_unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "unet_4d_atlantic_recall = (recall_eval(unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "am_unet_4d_atlantic_recall = (recall_eval(attention_unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_4d_atlantic_f1_score = (f1_score_eval_basic(unet_4d_atlantic_precision, unet_4d_atlantic_recall))\n",
        "am_unet_4d_atlantic_f1_score = (f1_score_eval_basic(am_unet_4d_atlantic_precision, am_unet_4d_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuoQ8__mE9Ut"
      },
      "source": [
        "# Print metrics\n",
        "print('U-Net score: ', np.mean(unet_4d_atlantic_score), np.std(unet_4d_atlantic_score))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_4d_atlantic_score), np.std(am_unet_4d_atlantic_score))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_4d_atlantic_precision), np.std(unet_4d_atlantic_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_atlantic_precision), np.std(am_unet_4d_atlantic_precision))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_4d_atlantic_recall), np.std(unet_4d_atlantic_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_atlantic_recall), np.std(am_unet_4d_atlantic_recall))\n",
        "\n",
        "print('U-Net F1-score: ', unet_4d_atlantic_f1_score)\n",
        "print('Attention U-Net F1-score: ', am_unet_4d_atlantic_f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LttV4BY-FO9Y"
      },
      "source": [
        "### Atlantic on unseen Amazon data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0mUcfs7FWCn"
      },
      "source": [
        "# Score\n",
        "unet_atlantic_on_amazon_score = score_eval2(unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2)\n",
        "am_unet_atlantic_on_amazon_score = score_eval2(attention_unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2)\n",
        "\n",
        "# Precision\n",
        "unet_atlantic_on_amazon_precision = (precision_eval(unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "am_unet_atlantic_on_amazon_precision = (precision_eval(attention_unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# Recall\n",
        "unet_atlantic_on_amazon_recall = (recall_eval(unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "am_unet_atlantic_on_amazon_recall = (recall_eval(attention_unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_atlantic_on_amazon_f1_score = (f1_score_eval_basic(unet_atlantic_on_amazon_precision, unet_atlantic_on_amazon_recall))\n",
        "am_unet_atlantic_on_amazon_f1_score = (f1_score_eval_basic(am_unet_atlantic_on_amazon_precision, am_unet_atlantic_on_amazon_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emaoDHfdFWfL"
      },
      "source": [
        "# Print metrics\n",
        "print('U-Net score: ', np.mean(unet_atlantic_on_amazon_score), np.std(unet_atlantic_on_amazon_score))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_atlantic_on_amazon_score), np.std(am_unet_atlantic_on_amazon_score))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_atlantic_on_amazon_precision), np.std(unet_atlantic_on_amazon_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_atlantic_on_amazon_precision), np.std(am_unet_atlantic_on_amazon_precision))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_atlantic_on_amazon_recall), np.std(unet_atlantic_on_amazon_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_atlantic_on_amazon_recall), np.std(am_unet_atlantic_on_amazon_recall))\n",
        "\n",
        "print('U-Net F1-score: ', unet_atlantic_on_amazon_f1_score)\n",
        "print('Attention U-Net F1-score: ', am_unet_atlantic_on_amazon_f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EWGiqFzlsrv"
      },
      "source": [
        "### Amazon and Atlantic unseen test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txs1Av6GpAlu"
      },
      "source": [
        "# Amazon trained model on Amazon test data\n",
        "# Scores of each model\n",
        "unet_4d_score_test = (score_eval2(unet_4d, test_images2, test_masks2))\n",
        "am_unet_4d_score_test = (score_eval2(attention_unet_4d, test_images2, test_masks2))\n",
        "\n",
        "# Precision and recall of each model\n",
        "unet_4d_precision_test = (precision_eval(unet_4d, test_images2, test_masks2))\n",
        "am_unet_4d_precision_test = (precision_eval(attention_unet_4d, test_images2, test_masks2))\n",
        "\n",
        "unet_4d_recall_test = (recall_eval(unet_4d, test_images2, test_masks2))\n",
        "am_unet_4d_recall_test = (recall_eval(attention_unet_4d, test_images2, test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_4d_f1_score_test = (f1_score_eval_basic(unet_4d_precision_test, unet_4d_recall_test))\n",
        "am_unet_4d_f1_score_test = (f1_score_eval_basic(am_unet_4d_precision_test, am_unet_4d_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njk-zCC7lw8_"
      },
      "source": [
        "# Atlantic trained model on Atlantic test data\n",
        "# Scores of each model\n",
        "unet_4d_atlantic_score_test = (score_eval2(unet_4d_atlantic, test_images3, test_masks3))\n",
        "am_unet_4d_atlantic_score_test = (score_eval2(attention_unet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "unet_4d_atlantic_precision_test = (precision_eval(unet_4d_atlantic, test_images3, test_masks3))\n",
        "am_unet_4d_atlantic_precision_test = (precision_eval(attention_unet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "unet_4d_atlantic_recall_test = (recall_eval(unet_4d_atlantic, test_images3, test_masks3))\n",
        "am_unet_4d_atlantic_recall_test = (recall_eval(attention_unet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_4d_atlantic_f1_score_test = (f1_score_eval_basic(unet_4d_atlantic_precision_test, unet_4d_atlantic_recall_test))\n",
        "am_unet_4d_atlantic_f1_score_test = (f1_score_eval_basic(am_unet_4d_atlantic_precision_test, am_unet_4d_atlantic_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSCJGrpkppSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "0fb863b4-fa84-4c51-f2a0-3e9dd6224944"
      },
      "source": [
        "# Print metrics for Amazon on Amazon Test set\n",
        "print('U-Net score: ', np.mean(unet_4d_score_test), np.std(unet_4d_score_test))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_4d_score_test), np.std(am_unet_4d_score_test))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_4d_precision_test), np.std(unet_4d_precision_test))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_precision_test), np.std(am_unet_4d_precision_test))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_4d_recall_test), np.std(unet_4d_recall_test))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_recall_test), np.std(am_unet_4d_recall_test))\n",
        "\n",
        "print('U-Net F1-score: ', unet_4d_f1_score_test)\n",
        "print('Attention U-Net F1-score: ', am_unet_4d_f1_score_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'unet_4d_score_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1636700236.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print metrics for Amazon on Amazon Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U-Net score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attention U-Net score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mam_unet_4d_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mam_unet_4d_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U-Net precision: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_precision_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_precision_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'unet_4d_score_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnXkLvzuqpYv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "326a1d11-efe6-4e0e-af8d-86bcebb63d16"
      },
      "source": [
        "# Print metrics for Atlantic on Atlantic Test set\n",
        "print('U-Net score: ', np.mean(unet_4d_atlantic_score_test), np.std(unet_4d_atlantic_score_test))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_4d_atlantic_score_test), np.std(am_unet_4d_atlantic_score_test))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_4d_atlantic_precision_test), np.std(unet_4d_atlantic_precision_test))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_atlantic_precision_test), np.std(am_unet_4d_atlantic_precision_test))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_4d_atlantic_recall_test), np.std(unet_4d_atlantic_recall_test))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_atlantic_recall_test), np.std(am_unet_4d_atlantic_recall_test))\n",
        "\n",
        "print('U-Net F1-score: ', unet_4d_atlantic_f1_score_test)\n",
        "print('Attention U-Net F1-score: ', am_unet_4d_atlantic_f1_score_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'unet_4d_atlantic_score_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2458042565.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print metrics for Atlantic on Atlantic Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U-Net score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_atlantic_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_atlantic_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attention U-Net score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mam_unet_4d_atlantic_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mam_unet_4d_atlantic_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U-Net precision: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_atlantic_precision_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_4d_atlantic_precision_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'unet_4d_atlantic_score_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsvOhqlr8d2V"
      },
      "source": [
        "## ResNet50-SegNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PenYqdMW8frD"
      },
      "source": [
        "# Load ResNet models and history stats\n",
        "resnet_3d = ResNet50SegNet()\n",
        "resnet_3d.load_weights('resnet50segnet-3d.hdf5')\n",
        "resnet_4d_amazon = ResNet50SegNet(input_size=(512,512,4))\n",
        "resnet_4d_amazon.load_weights('resnet50segnet-4d.hdf5')\n",
        "resnet_4d_atlantic = ResNet50SegNet(input_size=(512,512,4))\n",
        "resnet_4d_atlantic.load_weights('resnet50segnet-4d-atlantic.hdf5')\n",
        "\n",
        "resnet_3d_history = np.load('resnet50segnet-3d-history.npy', allow_pickle='TRUE').item()\n",
        "resnet50segnet_4d_amazon_history = np.load('resnet50segnet-4d-history.npy', allow_pickle='TRUE').item()\n",
        "resnet50segnet_4d_atlantic_history = np.load('resnet50segnet-4d-history-atlantic.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MywXNxy490t6"
      },
      "source": [
        "# Metrics of each model on respective datasets\n",
        "\n",
        "# Score\n",
        "resnet_3d_score = (score_eval(resnet_3d, validation_images, validation_masks))\n",
        "resnet_4d_amazon_score = (score_eval2(resnet_4d_amazon, validation_images2, validation_masks2))\n",
        "resnet_4d_atlantic_score = (score_eval2(resnet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# Precision and recall\n",
        "resnet_3d_precision = (precision_eval(resnet_3d, validation_images, validation_masks))\n",
        "resnet_4d_amazon_precision = (precision_eval(resnet_4d_amazon, validation_images2, validation_masks2))\n",
        "resnet_4d_atlantic_precision = (precision_eval(resnet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "resnet_3d_recall = (recall_eval(resnet_3d, validation_images, validation_masks))\n",
        "resnet_4d_amazon_recall = (recall_eval(resnet_4d_amazon, validation_images2, validation_masks2))\n",
        "resnet_4d_atlantic_recall = (recall_eval(resnet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# F1-score\n",
        "resnet_3d_f1_score = (f1_score_eval_basic(resnet_3d_precision, resnet_3d_recall))\n",
        "resnet_4d_amazon_f1_score = (f1_score_eval_basic(resnet_4d_amazon_precision, resnet_4d_amazon_recall))\n",
        "resnet_4d_atlantic_f1_score = (f1_score_eval_basic(resnet_4d_atlantic_precision, resnet_4d_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJUCREQf--Vm"
      },
      "source": [
        "# Metrics of 4-dim Amazon trained model on Atlantic data and vice versa\n",
        "\n",
        "# Score\n",
        "resnet_4d_amazon_on_atlantic_score = (score_eval2(resnet_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "resnet_4d_atlantic_on_amazon_score = (score_eval2(resnet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# Precision and recall\n",
        "resnet_4d_amazon_on_atlantic_precision = (precision_eval(resnet_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "resnet_4d_atlantic_on_amazon_precision = (precision_eval(resnet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "resnet_4d_amazon_on_atlantic_recall = (recall_eval(resnet_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "resnet_4d_atlantic_on_amazon_recall = (recall_eval(resnet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# F1-score\n",
        "resnet_4d_amazon_on_atlantic_f1_score = (f1_score_eval_basic(resnet_4d_amazon_on_atlantic_precision, resnet_4d_amazon_on_atlantic_recall))\n",
        "resnet_4d_atlantic_on_amazon_f1_score = (f1_score_eval_basic(resnet_4d_atlantic_on_amazon_precision, resnet_4d_atlantic_on_amazon_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngUhJxj7_xWf"
      },
      "source": [
        "# Print metrics\n",
        "print('Accuracy | Precision | Recall | F1-score')\n",
        "print('ResNet 3-dim: ', np.mean(resnet_3d_score), np.mean(resnet_3d_precision), np.mean(resnet_3d_recall), resnet_3d_f1_score)\n",
        "print('ResNet 3-dim: ', np.std(resnet_3d_score), np.std(resnet_3d_precision), np.std(resnet_3d_recall), resnet_3d_f1_score)\n",
        "print('----')\n",
        "print('ResNet 4-dim Amazon: ', np.mean(resnet_4d_amazon_score), np.mean(resnet_4d_amazon_precision), np.mean(resnet_4d_amazon_recall), resnet_4d_amazon_f1_score)\n",
        "print('ResNet 4-dim Amazon: ', np.std(resnet_4d_amazon_score), np.std(resnet_4d_amazon_precision), np.std(resnet_4d_amazon_recall), resnet_4d_amazon_f1_score)\n",
        "print('----')\n",
        "print('ResNet 4-dim Atlantic: ', np.mean(resnet_4d_atlantic_score), np.mean(resnet_4d_atlantic_precision), np.mean(resnet_4d_atlantic_recall), resnet_4d_atlantic_f1_score)\n",
        "print('ResNet 4-dim Atlantic: ', np.std(resnet_4d_atlantic_score), np.std(resnet_4d_atlantic_precision), np.std(resnet_4d_atlantic_recall), resnet_4d_atlantic_f1_score)\n",
        "print('----')\n",
        "print('ResNet 4-dim Amazon on Atlantic: ', np.mean(resnet_4d_amazon_on_atlantic_score), np.mean(resnet_4d_amazon_on_atlantic_precision), np.mean(resnet_4d_amazon_on_atlantic_recall), resnet_4d_amazon_on_atlantic_f1_score)\n",
        "print('ResNet 4-dim Amazon on Atlantic: ', np.std(resnet_4d_amazon_on_atlantic_score), np.std(resnet_4d_amazon_on_atlantic_precision), np.std(resnet_4d_amazon_on_atlantic_recall), resnet_4d_amazon_on_atlantic_f1_score)\n",
        "print('----')\n",
        "print('ResNet 4-dim Atlantic on Amazon: ', np.mean(resnet_4d_atlantic_on_amazon_score), np.mean(resnet_4d_atlantic_on_amazon_precision), np.mean(resnet_4d_atlantic_on_amazon_recall), resnet_4d_atlantic_on_amazon_f1_score)\n",
        "print('ResNet 4-dim Atlantic on Amazon: ', np.std(resnet_4d_atlantic_on_amazon_score), np.std(resnet_4d_atlantic_on_amazon_precision), np.std(resnet_4d_atlantic_on_amazon_recall), resnet_4d_atlantic_on_amazon_f1_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTWUmNO0lzYn"
      },
      "source": [
        "### Amazon and Atlantic unseen test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrWEhFiql0l3"
      },
      "source": [
        "# Amazon trained model on Amazon test data\n",
        "# Scores of each model\n",
        "resnet_4d_score_test = (score_eval2(resnet_4d_amazon, test_images2, test_masks2))\n",
        "\n",
        "# Precision and recall of each model\n",
        "resnet_4d_precision_test = (precision_eval(resnet_4d_amazon, test_images2, test_masks2))\n",
        "resnet_4d_recall_test = (recall_eval(resnet_4d_amazon, test_images2, test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "resnet_4d_f1_score_test = (f1_score_eval_basic(resnet_4d_precision_test, resnet_4d_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_UxTzoPrI1p"
      },
      "source": [
        "# Atlantic trained model on Atlantic test data\n",
        "# Scores of each model\n",
        "resnet_4d_atlantic_score_test = (score_eval2(resnet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "resnet_4d_atlantic_precision_test = (precision_eval(resnet_4d_atlantic, test_images3, test_masks3))\n",
        "resnet_4d_atlantic_recall_test = (recall_eval(resnet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "resnet_4d_atlantic_f1_score_test = (f1_score_eval_basic(resnet_4d_atlantic_precision_test, resnet_4d_atlantic_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye2nYpk6rKgz"
      },
      "source": [
        "# Print metrics for Amazon on Amazon Test set\n",
        "print('ResNet score: ', np.mean(resnet_4d_score_test), np.std(resnet_4d_score_test))\n",
        "print('ResNet precision: ', np.mean(resnet_4d_precision_test), np.std(resnet_4d_precision_test))\n",
        "print('ResNet recall: ', np.mean(resnet_4d_recall_test), np.std(resnet_4d_recall_test))\n",
        "print('ResNet F1-score: ', resnet_4d_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65sr0IQcrMbZ"
      },
      "source": [
        "# Print metrics for Atlantic on Atlantic Test set\n",
        "print('ResNet score: ', np.mean(resnet_4d_atlantic_score_test), np.std(resnet_4d_atlantic_score_test))\n",
        "print('ResNet precision: ', np.mean(resnet_4d_atlantic_precision_test), np.std(resnet_4d_atlantic_precision_test))\n",
        "print('ResNet recall: ', np.mean(resnet_4d_atlantic_recall_test), np.std(resnet_4d_atlantic_recall_test))\n",
        "print('ResNet F1-score: ', resnet_4d_atlantic_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f__KAFLHL7Nw"
      },
      "source": [
        "## FCN32-VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyHLMDbHL8-u"
      },
      "source": [
        "# Import models\n",
        "fcn32_3d = load_model('fcn32-3d.hdf5')\n",
        "fcn32_4d_amazon = load_model('fcn32-4d.hdf5')\n",
        "fcn32_4d_atlantic = load_model('fcn32-4d-atlantic.hdf5')\n",
        "\n",
        "fcn32_3d_history = np.load('fcn32-3d-history.npy', allow_pickle='TRUE').item()\n",
        "fcn32_4d_amazon_history = np.load('fcn32-4d-history.npy', allow_pickle='TRUE').item()\n",
        "fcn32_4d_atlantic_history = np.load('fcn32-4d-atlantic-history.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE6pvNzUM6Qj"
      },
      "source": [
        "# Metrics of each model on respective datasets\n",
        "\n",
        "# Score\n",
        "fcn32_3d_score = (score_eval(fcn32_3d, validation_images, validation_masks))\n",
        "fcn32_4d_amazon_score = (score_eval2(fcn32_4d_amazon, validation_images2, validation_masks2))\n",
        "fcn32_4d_atlantic_score = (score_eval2(fcn32_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# Precision and recall\n",
        "fcn32_3d_precision = (precision_eval(fcn32_3d, validation_images, validation_masks))\n",
        "fcn32_4d_amazon_precision = (precision_eval(fcn32_4d_amazon, validation_images2, validation_masks2))\n",
        "fcn32_4d_atlantic_precision = (precision_eval(fcn32_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "fcn32_3d_recall = (recall_eval(fcn32_3d, validation_images, validation_masks))\n",
        "fcn32_4d_amazon_recall = (recall_eval(fcn32_4d_amazon, validation_images2, validation_masks2))\n",
        "fcn32_4d_atlantic_recall = (recall_eval(fcn32_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# F1-score\n",
        "fcn32_3d_f1_score = (f1_score_eval_basic(fcn32_3d_precision, fcn32_3d_recall))\n",
        "fcn32_4d_amazon_f1_score = (f1_score_eval_basic(fcn32_4d_amazon_precision, fcn32_4d_amazon_recall))\n",
        "fcn32_4d_atlantic_f1_score = (f1_score_eval_basic(fcn32_4d_atlantic_precision, fcn32_4d_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx2ymcR3NLP_"
      },
      "source": [
        "# Metrics of 4-dim Amazon trained model on Atlantic data and vice versa\n",
        "\n",
        "# Score\n",
        "fcn32_4d_amazon_on_atlantic_score = (score_eval2(fcn32_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "fcn32_4d_atlantic_on_amazon_score = (score_eval2(fcn32_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# Precision and recall\n",
        "fcn32_4d_amazon_on_atlantic_precision = (precision_eval(fcn32_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "fcn32_4d_atlantic_on_amazon_precision = (precision_eval(fcn32_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "fcn32_4d_amazon_on_atlantic_recall = (recall_eval(fcn32_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "fcn32_4d_atlantic_on_amazon_recall = (recall_eval(fcn32_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# F1-score\n",
        "fcn32_4d_amazon_on_atlantic_f1_score = (f1_score_eval_basic(fcn32_4d_amazon_on_atlantic_precision, fcn32_4d_amazon_on_atlantic_recall))\n",
        "fcn32_4d_atlantic_on_amazon_f1_score = (f1_score_eval_basic(fcn32_4d_atlantic_on_amazon_precision, fcn32_4d_atlantic_on_amazon_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXe9ph53NXIb"
      },
      "source": [
        "# Print metrics\n",
        "print('Accuracy | Precision | Recall | F1-score')\n",
        "print('FCN32 3-dim: ', np.mean(fcn32_3d_score), np.mean(fcn32_3d_precision), np.mean(fcn32_3d_recall), fcn32_3d_f1_score)\n",
        "print('FCN32 3-dim: ', np.std(fcn32_3d_score), np.std(fcn32_3d_precision), np.std(fcn32_3d_recall), fcn32_3d_f1_score)\n",
        "print('----')\n",
        "print('FCN32 4-dim Amazon: ', np.mean(fcn32_4d_amazon_score), np.mean(fcn32_4d_amazon_precision), np.mean(fcn32_4d_amazon_recall), fcn32_4d_amazon_f1_score)\n",
        "print('FCN32 4-dim Amazon: ', np.std(fcn32_4d_amazon_score), np.std(fcn32_4d_amazon_precision), np.std(fcn32_4d_amazon_recall), fcn32_4d_amazon_f1_score)\n",
        "print('----')\n",
        "print('FCN32 4-dim Atlantic: ', np.mean(fcn32_4d_atlantic_score), np.mean(fcn32_4d_atlantic_precision), np.mean(fcn32_4d_atlantic_recall), fcn32_4d_atlantic_f1_score)\n",
        "print('FCN32 4-dim Atlantic: ', np.std(fcn32_4d_atlantic_score), np.std(fcn32_4d_atlantic_precision), np.std(fcn32_4d_atlantic_recall), fcn32_4d_atlantic_f1_score)\n",
        "print('----')\n",
        "print('FCN32 4-dim Amazon on Atlantic: ', np.mean(fcn32_4d_amazon_on_atlantic_score), np.mean(fcn32_4d_amazon_on_atlantic_precision), np.mean(fcn32_4d_amazon_on_atlantic_recall), fcn32_4d_amazon_on_atlantic_f1_score)\n",
        "print('FCN32 4-dim Amazon on Atlantic: ', np.std(fcn32_4d_amazon_on_atlantic_score), np.std(fcn32_4d_amazon_on_atlantic_precision), np.std(fcn32_4d_amazon_on_atlantic_recall), fcn32_4d_amazon_on_atlantic_f1_score)\n",
        "print('----')\n",
        "print('FCN32 4-dim Atlantic on Amazon: ', np.mean(fcn32_4d_atlantic_on_amazon_score), np.mean(fcn32_4d_atlantic_on_amazon_precision), np.mean(fcn32_4d_atlantic_on_amazon_recall), fcn32_4d_atlantic_on_amazon_f1_score)\n",
        "print('FCN32 4-dim Atlantic on Amazon: ', np.std(fcn32_4d_atlantic_on_amazon_score), np.std(fcn32_4d_atlantic_on_amazon_precision), np.std(fcn32_4d_atlantic_on_amazon_recall), fcn32_4d_atlantic_on_amazon_f1_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XExXvTaUl1hH"
      },
      "source": [
        "### Amazon and Atlantic unseen test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouL1rrHSl2RH"
      },
      "source": [
        "# Amazon trained model on Amazon test data\n",
        "# Scores of each model\n",
        "fcn32_4d_score_test = (score_eval2(fcn32_4d_amazon, test_images2, test_masks2))\n",
        "\n",
        "# Precision and recall of each model\n",
        "fcn32_4d_precision_test = (precision_eval(fcn32_4d_amazon, test_images2, test_masks2))\n",
        "fcn32_4d_recall_test = (recall_eval(fcn32_4d_amazon, test_images2, test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "fcn32_4d_f1_score_test = (f1_score_eval_basic(fcn32_4d_precision_test, fcn32_4d_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBvV64WrsVrT"
      },
      "source": [
        "# Atlantic trained model on Atlantic test data\n",
        "# Scores of each model\n",
        "fcn32_4d_atlantic_score_test = (score_eval2(fcn32_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "fcn32_4d_atlantic_precision_test = (precision_eval(fcn32_4d_atlantic, test_images3, test_masks3))\n",
        "fcn32_4d_atlantic_recall_test = (recall_eval(fcn32_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "fcn32_4d_atlantic_f1_score_test = (f1_score_eval_basic(fcn32_4d_atlantic_precision_test, fcn32_4d_atlantic_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq8c0dsQsXQs"
      },
      "source": [
        "# Print metrics for Amazon on Amazon Test set\n",
        "print('FCN32 score: ', np.mean(fcn32_4d_score_test), np.std(fcn32_4d_score_test))\n",
        "print('FCN32 precision: ', np.mean(fcn32_4d_precision_test), np.std(fcn32_4d_precision_test))\n",
        "print('FCN32 recall: ', np.mean(fcn32_4d_recall_test), np.std(fcn32_4d_recall_test))\n",
        "print('FCN32 F1-score: ', fcn32_4d_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu8TMJWRsZAg"
      },
      "source": [
        "# Print metrics for Atlantic on Atlantic Test set\n",
        "print('FCN32 score: ', np.mean(fcn32_4d_atlantic_score_test), np.std(fcn32_4d_atlantic_score_test))\n",
        "print('FCN32 precision: ', np.mean(fcn32_4d_atlantic_precision_test), np.std(fcn32_4d_atlantic_precision_test))\n",
        "print('FCN32 recall: ', np.mean(fcn32_4d_atlantic_recall_test), np.std(fcn32_4d_atlantic_recall_test))\n",
        "print('FCN32 F1-score: ', fcn32_4d_atlantic_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm5Y4hYEglRb"
      },
      "source": [
        "## ResUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3rm1x6RglRe"
      },
      "source": [
        "# Load ResUNet models and history stats\n",
        "# Import models\n",
        "resunet_3d = res_unet(512, [64, 128, 256, 512], 3, 3, 1)\n",
        "resunet_4d_amazon = res_unet(512, [64, 128, 256, 512], 3, 4, 1)\n",
        "resunet_4d_atlantic = res_unet(512, [64, 128, 256, 512], 3, 4, 1)\n",
        "resunet_3d.load_weights('resunet-3d.hdf5')\n",
        "resunet_4d_amazon.load_weights('resunet-4d.hdf5')\n",
        "resunet_4d_atlantic.load_weights('resunet-4d-atlantic.hdf5')\n",
        "\n",
        "resunet_3d_history = np.load('resunet-3d-history.npy', allow_pickle='TRUE').item()\n",
        "resunet_4d_amazon_history = np.load('resunet-4d-history.npy', allow_pickle='TRUE').item()\n",
        "resunet_4d_atlantic_history = np.load('resunet-4d-atlantic-history.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbXwCvXZglRh"
      },
      "source": [
        "# Metrics of each model on respective datasets\n",
        "\n",
        "# Score\n",
        "resunet_3d_score = (score_eval(resunet_3d, validation_images, validation_masks))\n",
        "resunet_4d_amazon_score = (score_eval2(resunet_4d_amazon, validation_images2, validation_masks2))\n",
        "resunet_4d_atlantic_score = (score_eval2(resunet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# Precision and recall\n",
        "resunet_3d_precision = (precision_eval(resunet_3d, validation_images, validation_masks))\n",
        "resunet_4d_amazon_precision = (precision_eval(resunet_4d_amazon, validation_images2, validation_masks2))\n",
        "resunet_4d_atlantic_precision = (precision_eval(resunet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "resunet_3d_recall = (recall_eval(resunet_3d, validation_images, validation_masks))\n",
        "resunet_4d_amazon_recall = (recall_eval(resunet_4d_amazon, validation_images2, validation_masks2))\n",
        "resunet_4d_atlantic_recall = (recall_eval(resunet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# F1-score\n",
        "resunet_3d_f1_score = (f1_score_eval_basic(resunet_3d_precision, resnet_3d_recall))\n",
        "resunet_4d_amazon_f1_score = (f1_score_eval_basic(resunet_4d_amazon_precision, resnet_4d_amazon_recall))\n",
        "resunet_4d_atlantic_f1_score = (f1_score_eval_basic(resunet_4d_atlantic_precision, resnet_4d_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj98qrhjglRl"
      },
      "source": [
        "# Metrics of 4-dim Amazon trained model on Atlantic data and vice versa\n",
        "\n",
        "# Score\n",
        "resunet_4d_amazon_on_atlantic_score = (score_eval2(resunet_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "resunet_4d_atlantic_on_amazon_score = (score_eval2(resunet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# Precision and recall\n",
        "resunet_4d_amazon_on_atlantic_precision = (precision_eval(resunet_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "resunet_4d_atlantic_on_amazon_precision = (precision_eval(resunet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "resunet_4d_amazon_on_atlantic_recall = (recall_eval(resunet_4d_amazon, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "resunet_4d_atlantic_on_amazon_recall = (recall_eval(resunet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# F1-score\n",
        "resunet_4d_amazon_on_atlantic_f1_score = (f1_score_eval_basic(resunet_4d_amazon_on_atlantic_precision, resunet_4d_amazon_on_atlantic_recall))\n",
        "resunet_4d_atlantic_on_amazon_f1_score = (f1_score_eval_basic(resunet_4d_atlantic_on_amazon_precision, resunet_4d_atlantic_on_amazon_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJIBcW91glRn"
      },
      "source": [
        "# Print metrics\n",
        "print('Accuracy | Precision | Recall | F1-score')\n",
        "print('ResUNet 3-dim: ', np.mean(resunet_3d_score), np.mean(resunet_3d_precision), np.mean(resunet_3d_recall), resunet_3d_f1_score)\n",
        "print('ResUNet 3-dim: ', np.std(resunet_3d_score), np.std(resunet_3d_precision), np.std(resunet_3d_recall), resunet_3d_f1_score)\n",
        "print('----')\n",
        "print('ResUNet 4-dim Amazon: ', np.mean(resunet_4d_amazon_score), np.mean(resunet_4d_amazon_precision), np.mean(resunet_4d_amazon_recall), resunet_4d_amazon_f1_score)\n",
        "print('ResUNet 4-dim Amazon: ', np.std(resunet_4d_amazon_score), np.std(resunet_4d_amazon_precision), np.std(resunet_4d_amazon_recall), resunet_4d_amazon_f1_score)\n",
        "print('----')\n",
        "print('ResUNet 4-dim Atlantic: ', np.mean(resunet_4d_atlantic_score), np.mean(resunet_4d_atlantic_precision), np.mean(resunet_4d_atlantic_recall), resunet_4d_atlantic_f1_score)\n",
        "print('ResUNet 4-dim Atlantic: ', np.std(resunet_4d_atlantic_score), np.std(resunet_4d_atlantic_precision), np.std(resunet_4d_atlantic_recall), resunet_4d_atlantic_f1_score)\n",
        "print('----')\n",
        "print('ResUNet 4-dim Amazon on Atlantic: ', np.mean(resunet_4d_amazon_on_atlantic_score), np.mean(resunet_4d_amazon_on_atlantic_precision), np.mean(resunet_4d_amazon_on_atlantic_recall), resunet_4d_amazon_on_atlantic_f1_score)\n",
        "print('ResUNet 4-dim Amazon on Atlantic: ', np.std(resunet_4d_amazon_on_atlantic_score), np.std(resunet_4d_amazon_on_atlantic_precision), np.std(resunet_4d_amazon_on_atlantic_recall), resunet_4d_amazon_on_atlantic_f1_score)\n",
        "print('----')\n",
        "print('ResUNet 4-dim Atlantic on Amazon: ', np.mean(resunet_4d_atlantic_on_amazon_score), np.mean(resunet_4d_atlantic_on_amazon_precision), np.mean(resunet_4d_atlantic_on_amazon_recall), resunet_4d_atlantic_on_amazon_f1_score)\n",
        "print('ResUNet 4-dim Atlantic on Amazon: ', np.std(resunet_4d_atlantic_on_amazon_score), np.std(resunet_4d_atlantic_on_amazon_precision), np.std(resunet_4d_atlantic_on_amazon_recall), resunet_4d_atlantic_on_amazon_f1_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuDsEgFoglRo"
      },
      "source": [
        "### Amazon and Atlantic unseen test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYJncND-glRq"
      },
      "source": [
        "# Amazon trained model on Amazon test data\n",
        "# Scores of each model\n",
        "resunet_4d_score_test = (score_eval2(resunet_4d_amazon, test_images2, test_masks2))\n",
        "\n",
        "# Precision and recall of each model\n",
        "resunet_4d_precision_test = (precision_eval(resunet_4d_amazon, test_images2, test_masks2))\n",
        "resunet_4d_recall_test = (recall_eval(resunet_4d_amazon, test_images2, test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "resunet_4d_f1_score_test = (f1_score_eval_basic(resunet_4d_precision_test, resunet_4d_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDtjq2hNglRr"
      },
      "source": [
        "# Atlantic trained model on Atlantic test data\n",
        "# Scores of each model\n",
        "resunet_4d_atlantic_score_test = (score_eval2(resunet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "resunet_4d_atlantic_precision_test = (precision_eval(resunet_4d_atlantic, test_images3, test_masks3))\n",
        "resunet_4d_atlantic_recall_test = (recall_eval(resunet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "resunet_4d_atlantic_f1_score_test = (f1_score_eval_basic(resunet_4d_atlantic_precision_test, resunet_4d_atlantic_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8II8kUwAglRt"
      },
      "source": [
        "# Print metrics for Amazon on Amazon Test set\n",
        "print('ResUNet score: ', np.mean(resunet_4d_score_test), np.std(resunet_4d_score_test))\n",
        "print('ResUNet precision: ', np.mean(resunet_4d_precision_test), np.std(resunet_4d_precision_test))\n",
        "print('ResUNet recall: ', np.mean(resunet_4d_recall_test), np.std(resunet_4d_recall_test))\n",
        "print('ResUNet F1-score: ', resunet_4d_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BTztlieglRu"
      },
      "source": [
        "# Print metrics for Atlantic on Atlantic Test set\n",
        "print('ResUNet score: ', np.mean(resunet_4d_atlantic_score_test), np.std(resunet_4d_atlantic_score_test))\n",
        "print('ResUNet precision: ', np.mean(resunet_4d_atlantic_precision_test), np.std(resunet_4d_atlantic_precision_test))\n",
        "print('ResUNet recall: ', np.mean(resunet_4d_atlantic_recall_test), np.std(resunet_4d_atlantic_recall_test))\n",
        "print('ResUNet F1-score: ', resunet_4d_atlantic_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PIo2l0n1XOt"
      },
      "source": [
        "# Produce metric datasets for export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1rZvCii11wn"
      },
      "source": [
        "## RGB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yTM5am91bWO"
      },
      "source": [
        "scores_3d = [unet_score, am_unet_score, resnet_3d_score, fcn32_3d_score, resunet_3d_score]\n",
        "precision_3d = [unet_precision, am_unet_precision, resnet_3d_precision, fcn32_3d_precision, resunet_3d_precision]\n",
        "recall_3d = [unet_recall, am_unet_recall, resnet_3d_recall, fcn32_3d_recall, resunet_3d_recall]\n",
        "f1_scores_3d = [unet_f1_score, am_unet_f1_score, resnet_3d_f1_score, fcn32_3d_f1_score, resunet_3d_f1_score]\n",
        "\n",
        "import pandas as pd\n",
        "metrics_3d = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_3d],\n",
        "              'precision': [np.mean(n) for n in precision_3d],\n",
        "              'recall': [np.mean(n) for n in recall_3d],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_3d],\n",
        "              'accuracy_std': [np.std(n) for n in scores_3d],\n",
        "              'precision_std': [np.std(n) for n in precision_3d],\n",
        "              'recall_std': [np.std(n) for n in recall_3d]\n",
        "              }\n",
        "metrics_3d = pd.DataFrame(metrics_3d)\n",
        "metrics_3d.to_csv('metrics_3d.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj2L_RE7149Q"
      },
      "source": [
        "## 4-band Amazon data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMbXcGo31682"
      },
      "source": [
        "scores_4d = [unet_4d_score, am_unet_4d_score, resnet_4d_amazon_score, fcn32_4d_amazon_score, resunet_4d_amazon_score]\n",
        "precision_4d = [unet_4d_precision, am_unet_4d_precision, resnet_4d_amazon_precision, fcn32_4d_amazon_precision, resunet_4d_amazon_precision]\n",
        "recall_4d = [unet_4d_recall, am_unet_4d_recall, resnet_4d_amazon_recall, fcn32_4d_amazon_recall, resunet_4d_amazon_recall]\n",
        "f1_scores_4d = [unet_4d_f1_score, am_unet_4d_f1_score, resnet_4d_amazon_f1_score, fcn32_4d_amazon_f1_score, resunet_4d_amazon_f1_score]\n",
        "\n",
        "metrics_4d = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d],\n",
        "              'precision': [np.mean(n) for n in precision_4d],\n",
        "              'recall': [np.mean(n) for n in recall_4d],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d],\n",
        "              'precision_std': [np.std(n) for n in precision_4d],\n",
        "              'recall_std': [np.std(n) for n in recall_4d]\n",
        "              }\n",
        "metrics_4d = pd.DataFrame(metrics_4d)\n",
        "metrics_4d.to_csv('metrics_4d_amazon.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eYZ0RkN17XE"
      },
      "source": [
        "## 4-band Atlantic Forest data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edIeklLV19Zi"
      },
      "source": [
        "scores_4d_atl = [unet_4d_atlantic_score, am_unet_4d_atlantic_score, resnet_4d_atlantic_score, fcn32_4d_atlantic_score, resunet_4d_atlantic_score]\n",
        "precision_4d_atl = [unet_4d_atlantic_precision, am_unet_4d_atlantic_precision, resnet_4d_atlantic_precision, fcn32_4d_atlantic_precision, resunet_4d_atlantic_precision]\n",
        "recall_4d_atl = [unet_4d_atlantic_recall, am_unet_4d_atlantic_recall, resnet_4d_atlantic_recall, fcn32_4d_atlantic_recall, resunet_4d_atlantic_recall]\n",
        "f1_scores_4d_atl = [unet_4d_atlantic_f1_score, am_unet_4d_atlantic_f1_score, resnet_4d_atlantic_f1_score, fcn32_4d_atlantic_f1_score, resunet_4d_atlantic_f1_score]\n",
        "\n",
        "metrics_4d_atl = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d_atl],\n",
        "              'precision': [np.mean(n) for n in precision_4d_atl],\n",
        "              'recall': [np.mean(n) for n in recall_4d_atl],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d_atl],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d_atl],\n",
        "              'precision_std': [np.std(n) for n in precision_4d_atl],\n",
        "              'recall_std': [np.std(n) for n in recall_4d_atl]\n",
        "              }\n",
        "metrics_4d_atl = pd.DataFrame(metrics_4d_atl)\n",
        "metrics_4d_atl.to_csv('metrics_4d_atlantic_forest.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF7HQghy5Qxz"
      },
      "source": [
        "## Test set data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCt9VZ3s5SSr"
      },
      "source": [
        "scores_4d_test = [unet_4d_score_test, am_unet_4d_score_test, resnet_4d_score_test, fcn32_4d_score_test, resunet_4d_score_test]\n",
        "precision_4d_test = [unet_4d_precision_test, am_unet_4d_precision_test, resnet_4d_precision_test, fcn32_4d_precision_test, resunet_4d_precision_test]\n",
        "recall_4d_test = [unet_4d_recall_test, am_unet_4d_recall_test, resnet_4d_recall_test, fcn32_4d_recall_test, resunet_4d_recall_test]\n",
        "f1_scores_4d_test = [unet_4d_f1_score_test, am_unet_4d_f1_score_test, resnet_4d_f1_score_test, fcn32_4d_f1_score_test, resunet_4d_f1_score_test]\n",
        "\n",
        "metrics_4d_test = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d_test],\n",
        "              'precision': [np.mean(n) for n in precision_4d_test],\n",
        "              'recall': [np.mean(n) for n in recall_4d_test],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d_test],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d_test],\n",
        "              'precision_std': [np.std(n) for n in precision_4d_test],\n",
        "              'recall_std': [np.std(n) for n in recall_4d_test]\n",
        "              }\n",
        "metrics_4d_test = pd.DataFrame(metrics_4d_test)\n",
        "metrics_4d_test.to_csv('metrics_4d_amazon_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M127Qw5A5xUX"
      },
      "source": [
        "scores_4d_atl_test = [unet_4d_atlantic_score_test, am_unet_4d_atlantic_score_test, resnet_4d_atlantic_score_test, fcn32_4d_atlantic_score_test, resunet_4d_atlantic_score_test]\n",
        "precision_4d_atl_test = [unet_4d_atlantic_precision_test, am_unet_4d_atlantic_precision_test, resnet_4d_atlantic_precision_test, fcn32_4d_atlantic_precision_test, resunet_4d_atlantic_precision_test]\n",
        "recall_4d_atl_test = [unet_4d_atlantic_recall_test, am_unet_4d_atlantic_recall_test, resnet_4d_atlantic_recall_test, fcn32_4d_atlantic_recall_test, resunet_4d_atlantic_recall_test]\n",
        "f1_scores_4d_atl_test = [unet_4d_atlantic_f1_score_test, am_unet_4d_atlantic_f1_score_test, resnet_4d_atlantic_f1_score_test, fcn32_4d_atlantic_f1_score_test, resunet_4d_atlantic_f1_score_test]\n",
        "\n",
        "metrics_4d_atl_test = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d_atl_test],\n",
        "              'precision': [np.mean(n) for n in precision_4d_atl_test],\n",
        "              'recall': [np.mean(n) for n in recall_4d_atl_test],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d_atl_test],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d_atl_test],\n",
        "              'precision_std': [np.std(n) for n in precision_4d_atl_test],\n",
        "              'recall_std': [np.std(n) for n in recall_4d_atl_test]\n",
        "              }\n",
        "metrics_4d_atl_test = pd.DataFrame(metrics_4d_atl_test)\n",
        "metrics_4d_atl_test.to_csv('metrics_4d_atlantic_forest_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYkobMR86TR6"
      },
      "source": [
        "## Testing on opposite dataset (e.g. train on Amazon, test on Atlantic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYz22lx-6X3V"
      },
      "source": [
        "scores_amazon_on_atlantic = [unet_amazon_on_atlantic_score, am_unet_amazon_on_atlantic_score, resnet_4d_amazon_on_atlantic_score, fcn32_4d_amazon_on_atlantic_score, resunet_4d_amazon_on_atlantic_score]\n",
        "precision_amazon_on_atlantic = [unet_amazon_on_atlantic_precision, am_unet_amazon_on_atlantic_precision, resnet_4d_amazon_on_atlantic_precision, fcn32_4d_amazon_on_atlantic_precision, resunet_4d_amazon_on_atlantic_precision]\n",
        "recall_amazon_on_atlantic = [unet_amazon_on_atlantic_recall, am_unet_amazon_on_atlantic_recall, resnet_4d_amazon_on_atlantic_recall, fcn32_4d_amazon_on_atlantic_recall, resunet_4d_amazon_on_atlantic_recall]\n",
        "f1_scores_amazon_on_atlantic = [unet_amazon_on_atlantic_f1_score, am_unet_amazon_on_atlantic_f1_score, resnet_4d_amazon_on_atlantic_f1_score, fcn32_4d_amazon_on_atlantic_f1_score, resunet_4d_amazon_on_atlantic_f1_score]\n",
        "\n",
        "metrics_4d_amazon_on_atlantic = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_amazon_on_atlantic],\n",
        "              'precision': [np.mean(n) for n in precision_amazon_on_atlantic],\n",
        "              'recall': [np.mean(n) for n in recall_amazon_on_atlantic],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_amazon_on_atlantic],\n",
        "              'accuracy_std': [np.std(n) for n in scores_amazon_on_atlantic],\n",
        "              'precision_std': [np.std(n) for n in precision_amazon_on_atlantic],\n",
        "              'recall_std': [np.std(n) for n in recall_amazon_on_atlantic]\n",
        "              }\n",
        "metrics_4d_amazon_on_atlantic = pd.DataFrame(metrics_4d_amazon_on_atlantic)\n",
        "metrics_4d_amazon_on_atlantic.to_csv('metrics_4d_amazon_on_atlantic.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OunQa23t7mwA"
      },
      "source": [
        "scores_atlantic_on_amazon = [unet_atlantic_on_amazon_score, am_unet_atlantic_on_amazon_score, resnet_4d_atlantic_on_amazon_score, fcn32_4d_atlantic_on_amazon_score, resunet_4d_atlantic_on_amazon_score]\n",
        "precision_atlantic_on_amazon = [unet_atlantic_on_amazon_precision, am_unet_atlantic_on_amazon_precision, resnet_4d_atlantic_on_amazon_precision, fcn32_4d_atlantic_on_amazon_precision, resunet_4d_atlantic_on_amazon_precision]\n",
        "recall_atlantic_on_amazon = [unet_atlantic_on_amazon_recall, am_unet_atlantic_on_amazon_recall, resnet_4d_atlantic_on_amazon_recall, fcn32_4d_atlantic_on_amazon_recall, resunet_4d_atlantic_on_amazon_recall]\n",
        "f1_scores_atlantic_on_amazon = [unet_atlantic_on_amazon_f1_score, am_unet_atlantic_on_amazon_f1_score, resnet_4d_atlantic_on_amazon_f1_score, fcn32_4d_atlantic_on_amazon_f1_score, resunet_4d_atlantic_on_amazon_f1_score]\n",
        "\n",
        "metrics_4d_atlantic_on_amazon = {'classifier': ['U-Net', 'Attention U-Net', 'ResNet50-SegNet', 'FCN32-VGG16', 'ResUNet'],\n",
        "              'accuracy': [np.mean(n) for n in scores_atlantic_on_amazon],\n",
        "              'precision': [np.mean(n) for n in precision_atlantic_on_amazon],\n",
        "              'recall': [np.mean(n) for n in recall_atlantic_on_amazon],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_atlantic_on_amazon],\n",
        "              'accuracy_std': [np.std(n) for n in scores_atlantic_on_amazon],\n",
        "              'precision_std': [np.std(n) for n in precision_atlantic_on_amazon],\n",
        "              'recall_std': [np.std(n) for n in recall_atlantic_on_amazon]\n",
        "              }\n",
        "metrics_4d_atlantic_on_amazon = pd.DataFrame(metrics_4d_atlantic_on_amazon)\n",
        "metrics_4d_atlantic_on_amazon.to_csv('metrics_4d_atlantic_on_amazon.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}